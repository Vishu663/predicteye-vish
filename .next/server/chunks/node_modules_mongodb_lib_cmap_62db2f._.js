module.exports = {

"[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.OP_MSG = exports.OP_COMPRESSED = exports.OP_DELETE = exports.OP_QUERY = exports.OP_INSERT = exports.OP_UPDATE = exports.OP_REPLY = exports.MIN_SUPPORTED_QE_SERVER_VERSION = exports.MIN_SUPPORTED_QE_WIRE_VERSION = exports.MAX_SUPPORTED_WIRE_VERSION = exports.MIN_SUPPORTED_WIRE_VERSION = exports.MAX_SUPPORTED_SERVER_VERSION = exports.MIN_SUPPORTED_SERVER_VERSION = void 0;
exports.MIN_SUPPORTED_SERVER_VERSION = '4.2';
exports.MAX_SUPPORTED_SERVER_VERSION = '8.0';
exports.MIN_SUPPORTED_WIRE_VERSION = 8;
exports.MAX_SUPPORTED_WIRE_VERSION = 25;
exports.MIN_SUPPORTED_QE_WIRE_VERSION = 21;
exports.MIN_SUPPORTED_QE_SERVER_VERSION = '7.0';
exports.OP_REPLY = 1;
exports.OP_UPDATE = 2001;
exports.OP_INSERT = 2002;
exports.OP_QUERY = 2004;
exports.OP_DELETE = 2006;
exports.OP_COMPRESSED = 2012;
exports.OP_MSG = 2013; //# sourceMappingURL=constants.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/on_demand/document.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.OnDemandDocument = void 0;
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
/** @internal */ class OnDemandDocument {
    constructor(/** BSON bytes, this document begins at offset */ bson, /** The start of the document */ offset = 0, /** If this is an embedded document, indicates if this was a BSON array */ isArray = false, /** If elements was already calculated */ elements){
        this.bson = bson;
        this.offset = offset;
        this.isArray = isArray;
        /**
         * Maps JS strings to elements and jsValues for speeding up subsequent lookups.
         * - If `false` then name does not exist in the BSON document
         * - If `CachedBSONElement` instance name exists
         * - If `cache[name].value == null` jsValue has not yet been parsed
         *   - Null/Undefined values do not get cached because they are zero-length values.
         */ this.cache = Object.create(null);
        /** Caches the index of elements that have been named */ this.indexFound = Object.create(null);
        this.elements = elements ?? (0, bson_1.parseToElementsToArray)(this.bson, offset);
    }
    /** Only supports basic latin strings */ isElementName(name, element) {
        const nameLength = element[2 /* BSONElementOffset.nameLength */ ];
        const nameOffset = element[1 /* BSONElementOffset.nameOffset */ ];
        if (name.length !== nameLength) return false;
        const nameEnd = nameOffset + nameLength;
        for(let byteIndex = nameOffset, charIndex = 0; charIndex < name.length && byteIndex < nameEnd; charIndex++, byteIndex++){
            if (this.bson[byteIndex] !== name.charCodeAt(charIndex)) return false;
        }
        return true;
    }
    /**
     * Seeks into the elements array for an element matching the given name.
     *
     * @remarks
     * Caching:
     * - Caches the existence of a property making subsequent look ups for non-existent properties return immediately
     * - Caches names mapped to elements to avoid reiterating the array and comparing the name again
     * - Caches the index at which an element has been found to prevent rechecking against elements already determined to belong to another name
     *
     * @param name - a basic latin string name of a BSON element
     * @returns
     */ getElement(name) {
        const cachedElement = this.cache[name];
        if (cachedElement === false) return null;
        if (cachedElement != null) {
            return cachedElement;
        }
        if (typeof name === 'number') {
            if (this.isArray) {
                if (name < this.elements.length) {
                    const element = this.elements[name];
                    const cachedElement = {
                        element,
                        value: undefined
                    };
                    this.cache[name] = cachedElement;
                    this.indexFound[name] = true;
                    return cachedElement;
                } else {
                    return null;
                }
            } else {
                return null;
            }
        }
        for(let index = 0; index < this.elements.length; index++){
            const element = this.elements[index];
            // skip this element if it has already been associated with a name
            if (!(index in this.indexFound) && this.isElementName(name, element)) {
                const cachedElement = {
                    element,
                    value: undefined
                };
                this.cache[name] = cachedElement;
                this.indexFound[index] = true;
                return cachedElement;
            }
        }
        this.cache[name] = false;
        return null;
    }
    toJSValue(element, as) {
        const type = element[0 /* BSONElementOffset.type */ ];
        const offset = element[3 /* BSONElementOffset.offset */ ];
        const length = element[4 /* BSONElementOffset.length */ ];
        if (as !== type) {
            return null;
        }
        switch(as){
            case bson_1.BSONType.null:
            case bson_1.BSONType.undefined:
                return null;
            case bson_1.BSONType.double:
                return (0, bson_1.getFloat64LE)(this.bson, offset);
            case bson_1.BSONType.int:
                return (0, bson_1.getInt32LE)(this.bson, offset);
            case bson_1.BSONType.long:
                return (0, bson_1.getBigInt64LE)(this.bson, offset);
            case bson_1.BSONType.bool:
                return Boolean(this.bson[offset]);
            case bson_1.BSONType.objectId:
                return new bson_1.ObjectId(this.bson.subarray(offset, offset + 12));
            case bson_1.BSONType.timestamp:
                return new bson_1.Timestamp((0, bson_1.getBigInt64LE)(this.bson, offset));
            case bson_1.BSONType.string:
                return (0, bson_1.toUTF8)(this.bson, offset + 4, offset + length - 1, false);
            case bson_1.BSONType.binData:
                {
                    const totalBinarySize = (0, bson_1.getInt32LE)(this.bson, offset);
                    const subType = this.bson[offset + 4];
                    if (subType === 2) {
                        const subType2BinarySize = (0, bson_1.getInt32LE)(this.bson, offset + 1 + 4);
                        if (subType2BinarySize < 0) throw new bson_1.BSONError('Negative binary type element size found for subtype 0x02');
                        if (subType2BinarySize > totalBinarySize - 4) throw new bson_1.BSONError('Binary type with subtype 0x02 contains too long binary size');
                        if (subType2BinarySize < totalBinarySize - 4) throw new bson_1.BSONError('Binary type with subtype 0x02 contains too short binary size');
                        return new bson_1.Binary(this.bson.subarray(offset + 1 + 4 + 4, offset + 1 + 4 + 4 + subType2BinarySize), 2);
                    }
                    return new bson_1.Binary(this.bson.subarray(offset + 1 + 4, offset + 1 + 4 + totalBinarySize), subType);
                }
            case bson_1.BSONType.date:
                // Pretend this is correct.
                return new Date(Number((0, bson_1.getBigInt64LE)(this.bson, offset)));
            case bson_1.BSONType.object:
                return new OnDemandDocument(this.bson, offset);
            case bson_1.BSONType.array:
                return new OnDemandDocument(this.bson, offset, true);
            default:
                throw new bson_1.BSONError(`Unsupported BSON type: ${as}`);
        }
    }
    /**
     * Returns the number of elements in this BSON document
     */ size() {
        return this.elements.length;
    }
    /**
     * Checks for the existence of an element by name.
     *
     * @remarks
     * Uses `getElement` with the expectation that will populate caches such that a `has` call
     * followed by a `getElement` call will not repeat the cost paid by the first look up.
     *
     * @param name - element name
     */ has(name) {
        const cachedElement = this.cache[name];
        if (cachedElement === false) return false;
        if (cachedElement != null) return true;
        return this.getElement(name) != null;
    }
    get(name, as, required) {
        const element = this.getElement(name);
        if (element == null) {
            if (required === true) {
                throw new bson_1.BSONError(`BSON element "${name}" is missing`);
            } else {
                return null;
            }
        }
        if (element.value == null) {
            const value = this.toJSValue(element.element, as);
            if (value == null) {
                if (required === true) {
                    throw new bson_1.BSONError(`BSON element "${name}" is missing`);
                } else {
                    return null;
                }
            }
            // It is important to never store null
            element.value = value;
        }
        return element.value;
    }
    getNumber(name, required) {
        const maybeBool = this.get(name, bson_1.BSONType.bool);
        const bool = maybeBool == null ? null : maybeBool ? 1 : 0;
        const maybeLong = this.get(name, bson_1.BSONType.long);
        const long = maybeLong == null ? null : Number(maybeLong);
        const result = bool ?? long ?? this.get(name, bson_1.BSONType.int) ?? this.get(name, bson_1.BSONType.double);
        if (required === true && result == null) {
            throw new bson_1.BSONError(`BSON element "${name}" is missing`);
        }
        return result;
    }
    /**
     * Deserialize this object, DOES NOT cache result so avoid multiple invocations
     * @param options - BSON deserialization options
     */ toObject(options) {
        return (0, bson_1.deserialize)(this.bson, {
            ...options,
            index: this.offset,
            allowObjectSmallerThanBufferSize: true
        });
    }
    /** Returns this document's bytes only */ toBytes() {
        const size = (0, bson_1.getInt32LE)(this.bson, this.offset);
        return this.bson.subarray(this.offset, this.offset + size);
    }
}
exports.OnDemandDocument = OnDemandDocument; //# sourceMappingURL=document.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientBulkWriteCursorResponse = exports.ExplainedCursorResponse = exports.CursorResponse = exports.MongoDBResponse = void 0;
exports.isErrorResponse = isErrorResponse;
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const document_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/on_demand/document.js [app-route] (ecmascript)");
/**
 * Accepts a BSON payload and checks for na "ok: 0" element.
 * This utility is intended to prevent calling response class constructors
 * that expect the result to be a success and demand certain properties to exist.
 *
 * For example, a cursor response always expects a cursor embedded document.
 * In order to write the class such that the properties reflect that assertion (non-null)
 * we cannot invoke the subclass constructor if the BSON represents an error.
 *
 * @param bytes - BSON document returned from the server
 */ function isErrorResponse(bson, elements) {
    for(let eIdx = 0; eIdx < elements.length; eIdx++){
        const element = elements[eIdx];
        if (element[2 /* BSONElementOffset.nameLength */ ] === 2) {
            const nameOffset = element[1 /* BSONElementOffset.nameOffset */ ];
            // 111 == "o", 107 == "k"
            if (bson[nameOffset] === 111 && bson[nameOffset + 1] === 107) {
                const valueOffset = element[3 /* BSONElementOffset.offset */ ];
                const valueLength = element[4 /* BSONElementOffset.length */ ];
                // If any byte in the length of the ok number (works for any type) is non zero,
                // then it is considered "ok: 1"
                for(let i = valueOffset; i < valueOffset + valueLength; i++){
                    if (bson[i] !== 0x00) return false;
                }
                return true;
            }
        }
    }
    return true;
}
/** @internal */ class MongoDBResponse extends document_1.OnDemandDocument {
    get(name, as, required) {
        try {
            return super.get(name, as, required);
        } catch (cause) {
            throw new error_1.MongoUnexpectedServerResponseError(cause.message, {
                cause
            });
        }
    }
    static is(value) {
        return value instanceof MongoDBResponse;
    }
    static make(bson) {
        const elements = (0, bson_1.parseToElementsToArray)(bson, 0);
        const isError = isErrorResponse(bson, elements);
        return isError ? new MongoDBResponse(bson, 0, false, elements) : new this(bson, 0, false, elements);
    }
    /**
     * Returns true iff:
     * - ok is 0 and the top-level code === 50
     * - ok is 1 and the writeErrors array contains a code === 50
     * - ok is 1 and the writeConcern object contains a code === 50
     */ get isMaxTimeExpiredError() {
        // {ok: 0, code: 50 ... }
        const isTopLevel = this.ok === 0 && this.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired;
        if (isTopLevel) return true;
        if (this.ok === 0) return false;
        // {ok: 1, writeConcernError: {code: 50 ... }}
        const isWriteConcern = this.get('writeConcernError', bson_1.BSONType.object)?.getNumber('code') === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired;
        if (isWriteConcern) return true;
        const writeErrors = this.get('writeErrors', bson_1.BSONType.array);
        if (writeErrors?.size()) {
            for(let i = 0; i < writeErrors.size(); i++){
                const isWriteError = writeErrors.get(i, bson_1.BSONType.object)?.getNumber('code') === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired;
                // {ok: 1, writeErrors: [{code: 50 ... }]}
                if (isWriteError) return true;
            }
        }
        return false;
    }
    /**
     * Drivers can safely assume that the `recoveryToken` field is always a BSON document but drivers MUST NOT modify the
     * contents of the document.
     */ get recoveryToken() {
        return this.get('recoveryToken', bson_1.BSONType.object)?.toObject({
            promoteValues: false,
            promoteLongs: false,
            promoteBuffers: false,
            validation: {
                utf8: true
            }
        }) ?? null;
    }
    /**
     * The server creates a cursor in response to a snapshot find/aggregate command and reports atClusterTime within the cursor field in the response.
     * For the distinct command the server adds a top-level atClusterTime field to the response.
     * The atClusterTime field represents the timestamp of the read and is guaranteed to be majority committed.
     */ get atClusterTime() {
        return this.get('cursor', bson_1.BSONType.object)?.get('atClusterTime', bson_1.BSONType.timestamp) ?? this.get('atClusterTime', bson_1.BSONType.timestamp);
    }
    get operationTime() {
        return this.get('operationTime', bson_1.BSONType.timestamp);
    }
    /** Normalizes whatever BSON value is "ok" to a JS number 1 or 0. */ get ok() {
        return this.getNumber('ok') ? 1 : 0;
    }
    get $err() {
        return this.get('$err', bson_1.BSONType.string);
    }
    get errmsg() {
        return this.get('errmsg', bson_1.BSONType.string);
    }
    get code() {
        return this.getNumber('code');
    }
    get $clusterTime() {
        if (!('clusterTime' in this)) {
            const clusterTimeDoc = this.get('$clusterTime', bson_1.BSONType.object);
            if (clusterTimeDoc == null) {
                this.clusterTime = null;
                return null;
            }
            const clusterTime = clusterTimeDoc.get('clusterTime', bson_1.BSONType.timestamp, true);
            const signature = clusterTimeDoc.get('signature', bson_1.BSONType.object)?.toObject();
            // @ts-expect-error: `signature` is incorrectly typed. It is public API.
            this.clusterTime = {
                clusterTime,
                signature
            };
        }
        return this.clusterTime ?? null;
    }
    toObject(options) {
        const exactBSONOptions = {
            ...(0, bson_1.pluckBSONSerializeOptions)(options ?? {}),
            validation: (0, bson_1.parseUtf8ValidationOption)(options)
        };
        return super.toObject(exactBSONOptions);
    }
}
exports.MongoDBResponse = MongoDBResponse;
// {ok:1}
MongoDBResponse.empty = new MongoDBResponse(new Uint8Array([
    13,
    0,
    0,
    0,
    16,
    111,
    107,
    0,
    1,
    0,
    0,
    0,
    0
]));
/** @internal */ class CursorResponse extends MongoDBResponse {
    constructor(){
        super(...arguments);
        this._batch = null;
        this.iterated = 0;
        this._encryptedBatch = null;
    }
    /**
     * This supports a feature of the FindCursor.
     * It is an optimization to avoid an extra getMore when the limit has been reached
     */ static get emptyGetMore() {
        return new CursorResponse((0, bson_1.serialize)({
            ok: 1,
            cursor: {
                id: 0n,
                nextBatch: []
            }
        }));
    }
    static is(value) {
        return value instanceof CursorResponse || value === CursorResponse.emptyGetMore;
    }
    get cursor() {
        return this.get('cursor', bson_1.BSONType.object, true);
    }
    get id() {
        try {
            return bson_1.Long.fromBigInt(this.cursor.get('id', bson_1.BSONType.long, true));
        } catch (cause) {
            throw new error_1.MongoUnexpectedServerResponseError(cause.message, {
                cause
            });
        }
    }
    get ns() {
        const namespace = this.cursor.get('ns', bson_1.BSONType.string);
        if (namespace != null) return (0, utils_1.ns)(namespace);
        return null;
    }
    get length() {
        return Math.max(this.batchSize - this.iterated, 0);
    }
    get encryptedBatch() {
        if (this.encryptedResponse == null) return null;
        if (this._encryptedBatch != null) return this._encryptedBatch;
        const cursor = this.encryptedResponse?.get('cursor', bson_1.BSONType.object);
        if (cursor?.has('firstBatch')) this._encryptedBatch = cursor.get('firstBatch', bson_1.BSONType.array, true);
        else if (cursor?.has('nextBatch')) this._encryptedBatch = cursor.get('nextBatch', bson_1.BSONType.array, true);
        else throw new error_1.MongoUnexpectedServerResponseError('Cursor document did not contain a batch');
        return this._encryptedBatch;
    }
    get batch() {
        if (this._batch != null) return this._batch;
        const cursor = this.cursor;
        if (cursor.has('firstBatch')) this._batch = cursor.get('firstBatch', bson_1.BSONType.array, true);
        else if (cursor.has('nextBatch')) this._batch = cursor.get('nextBatch', bson_1.BSONType.array, true);
        else throw new error_1.MongoUnexpectedServerResponseError('Cursor document did not contain a batch');
        return this._batch;
    }
    get batchSize() {
        return this.batch?.size();
    }
    get postBatchResumeToken() {
        return this.cursor.get('postBatchResumeToken', bson_1.BSONType.object)?.toObject({
            promoteValues: false,
            promoteLongs: false,
            promoteBuffers: false,
            validation: {
                utf8: true
            }
        }) ?? null;
    }
    shift(options) {
        if (this.iterated >= this.batchSize) {
            return null;
        }
        const result = this.batch.get(this.iterated, bson_1.BSONType.object, true) ?? null;
        const encryptedResult = this.encryptedBatch?.get(this.iterated, bson_1.BSONType.object, true) ?? null;
        this.iterated += 1;
        if (options?.raw) {
            return result.toBytes();
        } else {
            const object = result.toObject(options);
            if (encryptedResult) {
                (0, utils_1.decorateDecryptionResult)(object, encryptedResult.toObject(options), true);
            }
            return object;
        }
    }
    clear() {
        this.iterated = this.batchSize;
    }
}
exports.CursorResponse = CursorResponse;
/**
 * Explain responses have nothing to do with cursor responses
 * This class serves to temporarily avoid refactoring how cursors handle
 * explain responses which is to detect that the response is not cursor-like and return the explain
 * result as the "first and only" document in the "batch" and end the "cursor"
 */ class ExplainedCursorResponse extends CursorResponse {
    constructor(){
        super(...arguments);
        this.isExplain = true;
        this._length = 1;
    }
    get id() {
        return bson_1.Long.fromBigInt(0n);
    }
    get batchSize() {
        return 0;
    }
    get ns() {
        return null;
    }
    get length() {
        return this._length;
    }
    shift(options) {
        if (this._length === 0) return null;
        this._length -= 1;
        return this.toObject(options);
    }
}
exports.ExplainedCursorResponse = ExplainedCursorResponse;
/**
 * Client bulk writes have some extra metadata at the top level that needs to be
 * included in the result returned to the user.
 */ class ClientBulkWriteCursorResponse extends CursorResponse {
    get insertedCount() {
        return this.get('nInserted', bson_1.BSONType.int, true);
    }
    get upsertedCount() {
        return this.get('nUpserted', bson_1.BSONType.int, true);
    }
    get matchedCount() {
        return this.get('nMatched', bson_1.BSONType.int, true);
    }
    get modifiedCount() {
        return this.get('nModified', bson_1.BSONType.int, true);
    }
    get deletedCount() {
        return this.get('nDeleted', bson_1.BSONType.int, true);
    }
    get writeConcernError() {
        return this.get('writeConcernError', bson_1.BSONType.object, false);
    }
}
exports.ClientBulkWriteCursorResponse = ClientBulkWriteCursorResponse; //# sourceMappingURL=responses.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AuthProvider = exports.AuthContext = void 0;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
/**
 * Context used during authentication
 * @internal
 */ class AuthContext {
    constructor(connection, credentials, options){
        /** If the context is for reauthentication. */ this.reauthenticating = false;
        this.connection = connection;
        this.credentials = credentials;
        this.options = options;
    }
}
exports.AuthContext = AuthContext;
/**
 * Provider used during authentication.
 * @internal
 */ class AuthProvider {
    /**
     * Prepare the handshake document before the initial handshake.
     *
     * @param handshakeDoc - The document used for the initial handshake on a connection
     * @param authContext - Context for authentication flow
     */ async prepare(handshakeDoc, _authContext) {
        return handshakeDoc;
    }
    /**
     * Reauthenticate.
     * @param context - The shared auth context.
     */ async reauth(context) {
        if (context.reauthenticating) {
            throw new error_1.MongoRuntimeError('Reauthentication already in progress.');
        }
        try {
            context.reauthenticating = true;
            await this.auth(context);
        } finally{
            context.reauthenticating = false;
        }
    }
}
exports.AuthProvider = AuthProvider; //# sourceMappingURL=auth_provider.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/gssapi.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GSSAPI = exports.GSSAPICanonicalizationValue = void 0;
exports.performGSSAPICanonicalizeHostName = performGSSAPICanonicalizeHostName;
exports.resolveCname = resolveCname;
const dns = require("dns");
const deps_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/deps.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const auth_provider_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [app-route] (ecmascript)");
/** @public */ exports.GSSAPICanonicalizationValue = Object.freeze({
    on: true,
    off: false,
    none: 'none',
    forward: 'forward',
    forwardAndReverse: 'forwardAndReverse'
});
async function externalCommand(connection, command) {
    const response = await connection.command((0, utils_1.ns)('$external.$cmd'), command);
    return response;
}
let krb;
class GSSAPI extends auth_provider_1.AuthProvider {
    async auth(authContext) {
        const { connection, credentials } = authContext;
        if (credentials == null) {
            throw new error_1.MongoMissingCredentialsError('Credentials required for GSSAPI authentication');
        }
        const { username } = credentials;
        const client = await makeKerberosClient(authContext);
        const payload = await client.step('');
        const saslStartResponse = await externalCommand(connection, saslStart(payload));
        const negotiatedPayload = await negotiate(client, 10, saslStartResponse.payload);
        const saslContinueResponse = await externalCommand(connection, saslContinue(negotiatedPayload, saslStartResponse.conversationId));
        const finalizePayload = await finalize(client, username, saslContinueResponse.payload);
        await externalCommand(connection, {
            saslContinue: 1,
            conversationId: saslContinueResponse.conversationId,
            payload: finalizePayload
        });
    }
}
exports.GSSAPI = GSSAPI;
async function makeKerberosClient(authContext) {
    const { hostAddress } = authContext.options;
    const { credentials } = authContext;
    if (!hostAddress || typeof hostAddress.host !== 'string' || !credentials) {
        throw new error_1.MongoInvalidArgumentError('Connection must have host and port and credentials defined.');
    }
    loadKrb();
    if ('kModuleError' in krb) {
        throw krb['kModuleError'];
    }
    const { initializeClient } = krb;
    const { username, password } = credentials;
    const mechanismProperties = credentials.mechanismProperties;
    const serviceName = mechanismProperties.SERVICE_NAME ?? 'mongodb';
    const host = await performGSSAPICanonicalizeHostName(hostAddress.host, mechanismProperties);
    const initOptions = {};
    if (password != null) {
        // TODO(NODE-5139): These do not match the typescript options in initializeClient
        Object.assign(initOptions, {
            user: username,
            password: password
        });
    }
    const spnHost = mechanismProperties.SERVICE_HOST ?? host;
    let spn = `${serviceName}${("TURBOPACK compile-time truthy", 1) ? '/' : ("TURBOPACK unreachable", undefined)}${spnHost}`;
    if ('SERVICE_REALM' in mechanismProperties) {
        spn = `${spn}@${mechanismProperties.SERVICE_REALM}`;
    }
    return await initializeClient(spn, initOptions);
}
function saslStart(payload) {
    return {
        saslStart: 1,
        mechanism: 'GSSAPI',
        payload,
        autoAuthorize: 1
    };
}
function saslContinue(payload, conversationId) {
    return {
        saslContinue: 1,
        conversationId,
        payload
    };
}
async function negotiate(client, retries, payload) {
    try {
        const response = await client.step(payload);
        return response || '';
    } catch (error) {
        if (retries === 0) {
            // Retries exhausted, raise error
            throw error;
        }
        // Adjust number of retries and call step again
        return await negotiate(client, retries - 1, payload);
    }
}
async function finalize(client, user, payload) {
    // GSS Client Unwrap
    const response = await client.unwrap(payload);
    return await client.wrap(response || '', {
        user
    });
}
async function performGSSAPICanonicalizeHostName(host, mechanismProperties) {
    const mode = mechanismProperties.CANONICALIZE_HOST_NAME;
    if (!mode || mode === exports.GSSAPICanonicalizationValue.none) {
        return host;
    }
    // If forward and reverse or true
    if (mode === exports.GSSAPICanonicalizationValue.on || mode === exports.GSSAPICanonicalizationValue.forwardAndReverse) {
        // Perform the lookup of the ip address.
        const { address } = await dns.promises.lookup(host);
        try {
            // Perform a reverse ptr lookup on the ip address.
            const results = await dns.promises.resolvePtr(address);
            // If the ptr did not error but had no results, return the host.
            return results.length > 0 ? results[0] : host;
        } catch  {
            // This can error as ptr records may not exist for all ips. In this case
            // fallback to a cname lookup as dns.lookup() does not return the
            // cname.
            return await resolveCname(host);
        }
    } else {
        // The case for forward is just to resolve the cname as dns.lookup()
        // will not return it.
        return await resolveCname(host);
    }
}
async function resolveCname(host) {
    // Attempt to resolve the host name
    try {
        const results = await dns.promises.resolveCname(host);
        // Get the first resolved host id
        return results.length > 0 ? results[0] : host;
    } catch  {
        return host;
    }
}
/**
 * Load the Kerberos library.
 */ function loadKrb() {
    if (!krb) {
        krb = (0, deps_1.getKerberos)();
    }
} //# sourceMappingURL=gssapi.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/providers.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AUTH_MECHS_AUTH_SRC_EXTERNAL = exports.AuthMechanism = void 0;
/** @public */ exports.AuthMechanism = Object.freeze({
    MONGODB_AWS: 'MONGODB-AWS',
    MONGODB_CR: 'MONGODB-CR',
    MONGODB_DEFAULT: 'DEFAULT',
    MONGODB_GSSAPI: 'GSSAPI',
    MONGODB_PLAIN: 'PLAIN',
    MONGODB_SCRAM_SHA1: 'SCRAM-SHA-1',
    MONGODB_SCRAM_SHA256: 'SCRAM-SHA-256',
    MONGODB_X509: 'MONGODB-X509',
    MONGODB_OIDC: 'MONGODB-OIDC'
});
/** @internal */ exports.AUTH_MECHS_AUTH_SRC_EXTERNAL = new Set([
    exports.AuthMechanism.MONGODB_GSSAPI,
    exports.AuthMechanism.MONGODB_AWS,
    exports.AuthMechanism.MONGODB_OIDC,
    exports.AuthMechanism.MONGODB_X509
]); //# sourceMappingURL=providers.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongo_credentials.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoCredentials = exports.DEFAULT_ALLOWED_HOSTS = void 0;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const gssapi_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/gssapi.js [app-route] (ecmascript)");
const providers_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [app-route] (ecmascript)");
/**
 * @see https://github.com/mongodb/specifications/blob/master/source/auth/auth.md
 */ function getDefaultAuthMechanism(hello) {
    if (hello) {
        // If hello contains saslSupportedMechs, use scram-sha-256
        // if it is available, else scram-sha-1
        if (Array.isArray(hello.saslSupportedMechs)) {
            return hello.saslSupportedMechs.includes(providers_1.AuthMechanism.MONGODB_SCRAM_SHA256) ? providers_1.AuthMechanism.MONGODB_SCRAM_SHA256 : providers_1.AuthMechanism.MONGODB_SCRAM_SHA1;
        }
    }
    // Default auth mechanism for 4.0 and higher.
    return providers_1.AuthMechanism.MONGODB_SCRAM_SHA256;
}
const ALLOWED_ENVIRONMENT_NAMES = [
    'test',
    'azure',
    'gcp',
    'k8s'
];
const ALLOWED_HOSTS_ERROR = 'Auth mechanism property ALLOWED_HOSTS must be an array of strings.';
/** @internal */ exports.DEFAULT_ALLOWED_HOSTS = [
    '*.mongodb.net',
    '*.mongodb-qa.net',
    '*.mongodb-dev.net',
    '*.mongodbgov.net',
    'localhost',
    '127.0.0.1',
    '::1'
];
/** Error for when the token audience is missing in the environment. */ const TOKEN_RESOURCE_MISSING_ERROR = 'TOKEN_RESOURCE must be set in the auth mechanism properties when ENVIRONMENT is azure or gcp.';
/**
 * A representation of the credentials used by MongoDB
 * @public
 */ class MongoCredentials {
    constructor(options){
        this.username = options.username ?? '';
        this.password = options.password;
        this.source = options.source;
        if (!this.source && options.db) {
            this.source = options.db;
        }
        this.mechanism = options.mechanism || providers_1.AuthMechanism.MONGODB_DEFAULT;
        this.mechanismProperties = options.mechanismProperties || {};
        if (this.mechanism.match(/MONGODB-AWS/i)) {
            if (!this.username && process.env.AWS_ACCESS_KEY_ID) {
                this.username = process.env.AWS_ACCESS_KEY_ID;
            }
            if (!this.password && process.env.AWS_SECRET_ACCESS_KEY) {
                this.password = process.env.AWS_SECRET_ACCESS_KEY;
            }
            if (this.mechanismProperties.AWS_SESSION_TOKEN == null && process.env.AWS_SESSION_TOKEN != null) {
                this.mechanismProperties = {
                    ...this.mechanismProperties,
                    AWS_SESSION_TOKEN: process.env.AWS_SESSION_TOKEN
                };
            }
        }
        if (this.mechanism === providers_1.AuthMechanism.MONGODB_OIDC && !this.mechanismProperties.ALLOWED_HOSTS) {
            this.mechanismProperties = {
                ...this.mechanismProperties,
                ALLOWED_HOSTS: exports.DEFAULT_ALLOWED_HOSTS
            };
        }
        Object.freeze(this.mechanismProperties);
        Object.freeze(this);
    }
    /** Determines if two MongoCredentials objects are equivalent */ equals(other) {
        return this.mechanism === other.mechanism && this.username === other.username && this.password === other.password && this.source === other.source;
    }
    /**
     * If the authentication mechanism is set to "default", resolves the authMechanism
     * based on the server version and server supported sasl mechanisms.
     *
     * @param hello - A hello response from the server
     */ resolveAuthMechanism(hello) {
        // If the mechanism is not "default", then it does not need to be resolved
        if (this.mechanism.match(/DEFAULT/i)) {
            return new MongoCredentials({
                username: this.username,
                password: this.password,
                source: this.source,
                mechanism: getDefaultAuthMechanism(hello),
                mechanismProperties: this.mechanismProperties
            });
        }
        return this;
    }
    validate() {
        if ((this.mechanism === providers_1.AuthMechanism.MONGODB_GSSAPI || this.mechanism === providers_1.AuthMechanism.MONGODB_PLAIN || this.mechanism === providers_1.AuthMechanism.MONGODB_SCRAM_SHA1 || this.mechanism === providers_1.AuthMechanism.MONGODB_SCRAM_SHA256) && !this.username) {
            throw new error_1.MongoMissingCredentialsError(`Username required for mechanism '${this.mechanism}'`);
        }
        if (this.mechanism === providers_1.AuthMechanism.MONGODB_OIDC) {
            if (this.username && this.mechanismProperties.ENVIRONMENT && this.mechanismProperties.ENVIRONMENT !== 'azure') {
                throw new error_1.MongoInvalidArgumentError(`username and ENVIRONMENT '${this.mechanismProperties.ENVIRONMENT}' may not be used together for mechanism '${this.mechanism}'.`);
            }
            if (this.username && this.password) {
                throw new error_1.MongoInvalidArgumentError(`No password is allowed in ENVIRONMENT '${this.mechanismProperties.ENVIRONMENT}' for '${this.mechanism}'.`);
            }
            if ((this.mechanismProperties.ENVIRONMENT === 'azure' || this.mechanismProperties.ENVIRONMENT === 'gcp') && !this.mechanismProperties.TOKEN_RESOURCE) {
                throw new error_1.MongoInvalidArgumentError(TOKEN_RESOURCE_MISSING_ERROR);
            }
            if (this.mechanismProperties.ENVIRONMENT && !ALLOWED_ENVIRONMENT_NAMES.includes(this.mechanismProperties.ENVIRONMENT)) {
                throw new error_1.MongoInvalidArgumentError(`Currently only a ENVIRONMENT in ${ALLOWED_ENVIRONMENT_NAMES.join(',')} is supported for mechanism '${this.mechanism}'.`);
            }
            if (!this.mechanismProperties.ENVIRONMENT && !this.mechanismProperties.OIDC_CALLBACK && !this.mechanismProperties.OIDC_HUMAN_CALLBACK) {
                throw new error_1.MongoInvalidArgumentError(`Either a ENVIRONMENT, OIDC_CALLBACK, or OIDC_HUMAN_CALLBACK must be specified for mechanism '${this.mechanism}'.`);
            }
            if (this.mechanismProperties.ALLOWED_HOSTS) {
                const hosts = this.mechanismProperties.ALLOWED_HOSTS;
                if (!Array.isArray(hosts)) {
                    throw new error_1.MongoInvalidArgumentError(ALLOWED_HOSTS_ERROR);
                }
                for (const host of hosts){
                    if (typeof host !== 'string') {
                        throw new error_1.MongoInvalidArgumentError(ALLOWED_HOSTS_ERROR);
                    }
                }
            }
        }
        if (providers_1.AUTH_MECHS_AUTH_SRC_EXTERNAL.has(this.mechanism)) {
            if (this.source != null && this.source !== '$external') {
                // TODO(NODE-3485): Replace this with a MongoAuthValidationError
                throw new error_1.MongoAPIError(`Invalid source '${this.source}' for mechanism '${this.mechanism}' specified.`);
            }
        }
        if (this.mechanism === providers_1.AuthMechanism.MONGODB_PLAIN && this.source == null) {
            // TODO(NODE-3485): Replace this with a MongoAuthValidationError
            throw new error_1.MongoAPIError('PLAIN Authentication Mechanism needs an auth source');
        }
        if (this.mechanism === providers_1.AuthMechanism.MONGODB_X509 && this.password != null) {
            if (this.password === '') {
                Reflect.set(this, 'password', undefined);
                return;
            }
            // TODO(NODE-3485): Replace this with a MongoAuthValidationError
            throw new error_1.MongoAPIError(`Password not allowed for mechanism MONGODB-X509`);
        }
        const canonicalization = this.mechanismProperties.CANONICALIZE_HOST_NAME ?? false;
        if (!Object.values(gssapi_1.GSSAPICanonicalizationValue).includes(canonicalization)) {
            throw new error_1.MongoAPIError(`Invalid CANONICALIZE_HOST_NAME value: ${canonicalization}`);
        }
    }
    static merge(creds, options) {
        return new MongoCredentials({
            username: options.username ?? creds?.username ?? '',
            password: options.password ?? creds?.password ?? '',
            mechanism: options.mechanism ?? creds?.mechanism ?? providers_1.AuthMechanism.MONGODB_DEFAULT,
            mechanismProperties: options.mechanismProperties ?? creds?.mechanismProperties ?? {},
            source: options.source ?? options.db ?? creds?.source ?? 'admin'
        });
    }
}
exports.MongoCredentials = MongoCredentials; //# sourceMappingURL=mongo_credentials.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/handshake/client_metadata.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.LimitedSizeDocument = void 0;
exports.makeClientMetadata = makeClientMetadata;
exports.addContainerMetadata = addContainerMetadata;
exports.getFAASEnv = getFAASEnv;
const os = require("os");
const process = require("process");
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
// eslint-disable-next-line @typescript-eslint/no-require-imports
const NODE_DRIVER_VERSION = __turbopack_require__("[project]/node_modules/mongodb/package.json (json)").version;
/** @internal */ class LimitedSizeDocument {
    constructor(maxSize){
        this.maxSize = maxSize;
        this.document = new Map();
        /** BSON overhead: Int32 + Null byte */ this.documentSize = 5;
    }
    /** Only adds key/value if the bsonByteLength is less than MAX_SIZE */ ifItFitsItSits(key, value) {
        // The BSON byteLength of the new element is the same as serializing it to its own document
        // subtracting the document size int32 and the null terminator.
        const newElementSize = bson_1.BSON.serialize(new Map().set(key, value)).byteLength - 5;
        if (newElementSize + this.documentSize > this.maxSize) {
            return false;
        }
        this.documentSize += newElementSize;
        this.document.set(key, value);
        return true;
    }
    toObject() {
        return bson_1.BSON.deserialize(bson_1.BSON.serialize(this.document), {
            promoteLongs: false,
            promoteBuffers: false,
            promoteValues: false,
            useBigInt64: false
        });
    }
}
exports.LimitedSizeDocument = LimitedSizeDocument;
/**
 * From the specs:
 * Implementors SHOULD cumulatively update fields in the following order until the document is under the size limit:
 * 1. Omit fields from `env` except `env.name`.
 * 2. Omit fields from `os` except `os.type`.
 * 3. Omit the `env` document entirely.
 * 4. Truncate `platform`. -- special we do not truncate this field
 */ function makeClientMetadata(options) {
    const metadataDocument = new LimitedSizeDocument(512);
    const { appName = '' } = options;
    // Add app name first, it must be sent
    if (appName.length > 0) {
        const name = Buffer.byteLength(appName, 'utf8') <= 128 ? options.appName : Buffer.from(appName, 'utf8').subarray(0, 128).toString('utf8');
        metadataDocument.ifItFitsItSits('application', {
            name
        });
    }
    const { name = '', version = '', platform = '' } = options.driverInfo;
    const driverInfo = {
        name: name.length > 0 ? `nodejs|${name}` : 'nodejs',
        version: version.length > 0 ? `${NODE_DRIVER_VERSION}|${version}` : NODE_DRIVER_VERSION
    };
    if (!metadataDocument.ifItFitsItSits('driver', driverInfo)) {
        throw new error_1.MongoInvalidArgumentError('Unable to include driverInfo name and version, metadata cannot exceed 512 bytes');
    }
    let runtimeInfo = getRuntimeInfo();
    if (platform.length > 0) {
        runtimeInfo = `${runtimeInfo}|${platform}`;
    }
    if (!metadataDocument.ifItFitsItSits('platform', runtimeInfo)) {
        throw new error_1.MongoInvalidArgumentError('Unable to include driverInfo platform, metadata cannot exceed 512 bytes');
    }
    // Note: order matters, os.type is last so it will be removed last if we're at maxSize
    const osInfo = new Map().set('name', process.platform).set('architecture', process.arch).set('version', os.release()).set('type', os.type());
    if (!metadataDocument.ifItFitsItSits('os', osInfo)) {
        for (const key of osInfo.keys()){
            osInfo.delete(key);
            if (osInfo.size === 0) break;
            if (metadataDocument.ifItFitsItSits('os', osInfo)) break;
        }
    }
    const faasEnv = getFAASEnv();
    if (faasEnv != null) {
        if (!metadataDocument.ifItFitsItSits('env', faasEnv)) {
            for (const key of faasEnv.keys()){
                faasEnv.delete(key);
                if (faasEnv.size === 0) break;
                if (metadataDocument.ifItFitsItSits('env', faasEnv)) break;
            }
        }
    }
    return metadataDocument.toObject();
}
let dockerPromise;
/** @internal */ async function getContainerMetadata() {
    const containerMetadata = {};
    dockerPromise ??= (0, utils_1.fileIsAccessible)('/.dockerenv');
    const isDocker = await dockerPromise;
    const { KUBERNETES_SERVICE_HOST = '' } = process.env;
    const isKubernetes = KUBERNETES_SERVICE_HOST.length > 0 ? true : false;
    if (isDocker) containerMetadata.runtime = 'docker';
    if (isKubernetes) containerMetadata.orchestrator = 'kubernetes';
    return containerMetadata;
}
/**
 * @internal
 * Re-add each metadata value.
 * Attempt to add new env container metadata, but keep old data if it does not fit.
 */ async function addContainerMetadata(originalMetadata) {
    const containerMetadata = await getContainerMetadata();
    if (Object.keys(containerMetadata).length === 0) return originalMetadata;
    const extendedMetadata = new LimitedSizeDocument(512);
    const extendedEnvMetadata = {
        ...originalMetadata?.env,
        container: containerMetadata
    };
    for (const [key, val] of Object.entries(originalMetadata)){
        if (key !== 'env') {
            extendedMetadata.ifItFitsItSits(key, val);
        } else {
            if (!extendedMetadata.ifItFitsItSits('env', extendedEnvMetadata)) {
                // add in old data if newer / extended metadata does not fit
                extendedMetadata.ifItFitsItSits('env', val);
            }
        }
    }
    if (!('env' in originalMetadata)) {
        extendedMetadata.ifItFitsItSits('env', extendedEnvMetadata);
    }
    return extendedMetadata.toObject();
}
/**
 * Collects FaaS metadata.
 * - `name` MUST be the last key in the Map returned.
 */ function getFAASEnv() {
    const { AWS_EXECUTION_ENV = '', AWS_LAMBDA_RUNTIME_API = '', FUNCTIONS_WORKER_RUNTIME = '', K_SERVICE = '', FUNCTION_NAME = '', VERCEL = '', AWS_LAMBDA_FUNCTION_MEMORY_SIZE = '', AWS_REGION = '', FUNCTION_MEMORY_MB = '', FUNCTION_REGION = '', FUNCTION_TIMEOUT_SEC = '', VERCEL_REGION = '' } = process.env;
    const isAWSFaaS = AWS_EXECUTION_ENV.startsWith('AWS_Lambda_') || AWS_LAMBDA_RUNTIME_API.length > 0;
    const isAzureFaaS = FUNCTIONS_WORKER_RUNTIME.length > 0;
    const isGCPFaaS = K_SERVICE.length > 0 || FUNCTION_NAME.length > 0;
    const isVercelFaaS = VERCEL.length > 0;
    // Note: order matters, name must always be the last key
    const faasEnv = new Map();
    // When isVercelFaaS is true so is isAWSFaaS; Vercel inherits the AWS env
    if (isVercelFaaS && !(isAzureFaaS || isGCPFaaS)) {
        if (VERCEL_REGION.length > 0) {
            faasEnv.set('region', VERCEL_REGION);
        }
        faasEnv.set('name', 'vercel');
        return faasEnv;
    }
    if (isAWSFaaS && !(isAzureFaaS || isGCPFaaS || isVercelFaaS)) {
        if (AWS_REGION.length > 0) {
            faasEnv.set('region', AWS_REGION);
        }
        if (AWS_LAMBDA_FUNCTION_MEMORY_SIZE.length > 0 && Number.isInteger(+AWS_LAMBDA_FUNCTION_MEMORY_SIZE)) {
            faasEnv.set('memory_mb', new bson_1.Int32(AWS_LAMBDA_FUNCTION_MEMORY_SIZE));
        }
        faasEnv.set('name', 'aws.lambda');
        return faasEnv;
    }
    if (isAzureFaaS && !(isGCPFaaS || isAWSFaaS || isVercelFaaS)) {
        faasEnv.set('name', 'azure.func');
        return faasEnv;
    }
    if (isGCPFaaS && !(isAzureFaaS || isAWSFaaS || isVercelFaaS)) {
        if (FUNCTION_REGION.length > 0) {
            faasEnv.set('region', FUNCTION_REGION);
        }
        if (FUNCTION_MEMORY_MB.length > 0 && Number.isInteger(+FUNCTION_MEMORY_MB)) {
            faasEnv.set('memory_mb', new bson_1.Int32(FUNCTION_MEMORY_MB));
        }
        if (FUNCTION_TIMEOUT_SEC.length > 0 && Number.isInteger(+FUNCTION_TIMEOUT_SEC)) {
            faasEnv.set('timeout_sec', new bson_1.Int32(FUNCTION_TIMEOUT_SEC));
        }
        faasEnv.set('name', 'gcp.func');
        return faasEnv;
    }
    return null;
}
/**
 * @internal
 * Get current JavaScript runtime platform
 *
 * NOTE: The version information fetching is intentionally written defensively
 * to avoid having a released driver version that becomes incompatible
 * with a future change to these global objects.
 */ function getRuntimeInfo() {
    if ('Deno' in globalThis) {
        const version = typeof Deno?.version?.deno === 'string' ? Deno?.version?.deno : '0.0.0-unknown';
        return `Deno v${version}, ${os.endianness()}`;
    }
    if ('Bun' in globalThis) {
        const version = typeof Bun?.version === 'string' ? Bun?.version : '0.0.0-unknown';
        return `Bun v${version}, ${os.endianness()}`;
    }
    return `Node.js ${process.version}, ${os.endianness()}`;
} //# sourceMappingURL=client_metadata.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/commands.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.OpCompressedRequest = exports.OpMsgResponse = exports.OpMsgRequest = exports.DocumentSequence = exports.OpReply = exports.OpQueryRequest = void 0;
const BSON = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const compression_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [app-route] (ecmascript)");
const constants_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [app-route] (ecmascript)");
// Incrementing request id
let _requestId = 0;
// Query flags
const OPTS_TAILABLE_CURSOR = 2;
const OPTS_SECONDARY = 4;
const OPTS_OPLOG_REPLAY = 8;
const OPTS_NO_CURSOR_TIMEOUT = 16;
const OPTS_AWAIT_DATA = 32;
const OPTS_EXHAUST = 64;
const OPTS_PARTIAL = 128;
// Response flags
const CURSOR_NOT_FOUND = 1;
const QUERY_FAILURE = 2;
const SHARD_CONFIG_STALE = 4;
const AWAIT_CAPABLE = 8;
const encodeUTF8Into = BSON.BSON.onDemand.ByteUtils.encodeUTF8Into;
/** @internal */ class OpQueryRequest {
    constructor(databaseName, query, options){
        this.databaseName = databaseName;
        this.query = query;
        /** moreToCome is an OP_MSG only concept */ this.moreToCome = false;
        // Basic options needed to be passed in
        // TODO(NODE-3483): Replace with MongoCommandError
        const ns = `${databaseName}.$cmd`;
        if (typeof databaseName !== 'string') {
            throw new error_1.MongoRuntimeError('Database name must be a string for a query');
        }
        // TODO(NODE-3483): Replace with MongoCommandError
        if (query == null) throw new error_1.MongoRuntimeError('A query document must be specified for query');
        // Validate that we are not passing 0x00 in the collection name
        if (ns.indexOf('\x00') !== -1) {
            // TODO(NODE-3483): Use MongoNamespace static method
            throw new error_1.MongoRuntimeError('Namespace cannot contain a null character');
        }
        // Basic options
        this.ns = ns;
        // Additional options
        this.numberToSkip = options.numberToSkip || 0;
        this.numberToReturn = options.numberToReturn || 0;
        this.returnFieldSelector = options.returnFieldSelector || undefined;
        this.requestId = options.requestId ?? OpQueryRequest.getRequestId();
        // special case for pre-3.2 find commands, delete ASAP
        this.pre32Limit = options.pre32Limit;
        // Serialization option
        this.serializeFunctions = typeof options.serializeFunctions === 'boolean' ? options.serializeFunctions : false;
        this.ignoreUndefined = typeof options.ignoreUndefined === 'boolean' ? options.ignoreUndefined : false;
        this.maxBsonSize = options.maxBsonSize || 1024 * 1024 * 16;
        this.checkKeys = typeof options.checkKeys === 'boolean' ? options.checkKeys : false;
        this.batchSize = this.numberToReturn;
        // Flags
        this.tailable = false;
        this.secondaryOk = typeof options.secondaryOk === 'boolean' ? options.secondaryOk : false;
        this.oplogReplay = false;
        this.noCursorTimeout = false;
        this.awaitData = false;
        this.exhaust = false;
        this.partial = false;
    }
    /** Assign next request Id. */ incRequestId() {
        this.requestId = _requestId++;
    }
    /** Peek next request Id. */ nextRequestId() {
        return _requestId + 1;
    }
    /** Increment then return next request Id. */ static getRequestId() {
        return ++_requestId;
    }
    // Uses a single allocated buffer for the process, avoiding multiple memory allocations
    toBin() {
        const buffers = [];
        let projection = null;
        // Set up the flags
        let flags = 0;
        if (this.tailable) {
            flags |= OPTS_TAILABLE_CURSOR;
        }
        if (this.secondaryOk) {
            flags |= OPTS_SECONDARY;
        }
        if (this.oplogReplay) {
            flags |= OPTS_OPLOG_REPLAY;
        }
        if (this.noCursorTimeout) {
            flags |= OPTS_NO_CURSOR_TIMEOUT;
        }
        if (this.awaitData) {
            flags |= OPTS_AWAIT_DATA;
        }
        if (this.exhaust) {
            flags |= OPTS_EXHAUST;
        }
        if (this.partial) {
            flags |= OPTS_PARTIAL;
        }
        // If batchSize is different to this.numberToReturn
        if (this.batchSize !== this.numberToReturn) this.numberToReturn = this.batchSize;
        // Allocate write protocol header buffer
        const header = Buffer.alloc(4 * 4 + // Header
        4 + // Flags
        Buffer.byteLength(this.ns) + 1 + // namespace
        4 + // numberToSkip
        4 // numberToReturn
        );
        // Add header to buffers
        buffers.push(header);
        // Serialize the query
        const query = BSON.serialize(this.query, {
            checkKeys: this.checkKeys,
            serializeFunctions: this.serializeFunctions,
            ignoreUndefined: this.ignoreUndefined
        });
        // Add query document
        buffers.push(query);
        if (this.returnFieldSelector && Object.keys(this.returnFieldSelector).length > 0) {
            // Serialize the projection document
            projection = BSON.serialize(this.returnFieldSelector, {
                checkKeys: this.checkKeys,
                serializeFunctions: this.serializeFunctions,
                ignoreUndefined: this.ignoreUndefined
            });
            // Add projection document
            buffers.push(projection);
        }
        // Total message size
        const totalLength = header.length + query.length + (projection ? projection.length : 0);
        // Set up the index
        let index = 4;
        // Write total document length
        header[3] = totalLength >> 24 & 0xff;
        header[2] = totalLength >> 16 & 0xff;
        header[1] = totalLength >> 8 & 0xff;
        header[0] = totalLength & 0xff;
        // Write header information requestId
        header[index + 3] = this.requestId >> 24 & 0xff;
        header[index + 2] = this.requestId >> 16 & 0xff;
        header[index + 1] = this.requestId >> 8 & 0xff;
        header[index] = this.requestId & 0xff;
        index = index + 4;
        // Write header information responseTo
        header[index + 3] = 0 >> 24 & 0xff;
        header[index + 2] = 0 >> 16 & 0xff;
        header[index + 1] = 0 >> 8 & 0xff;
        header[index] = 0 & 0xff;
        index = index + 4;
        // Write header information OP_QUERY
        header[index + 3] = constants_1.OP_QUERY >> 24 & 0xff;
        header[index + 2] = constants_1.OP_QUERY >> 16 & 0xff;
        header[index + 1] = constants_1.OP_QUERY >> 8 & 0xff;
        header[index] = constants_1.OP_QUERY & 0xff;
        index = index + 4;
        // Write header information flags
        header[index + 3] = flags >> 24 & 0xff;
        header[index + 2] = flags >> 16 & 0xff;
        header[index + 1] = flags >> 8 & 0xff;
        header[index] = flags & 0xff;
        index = index + 4;
        // Write collection name
        index = index + header.write(this.ns, index, 'utf8') + 1;
        header[index - 1] = 0;
        // Write header information flags numberToSkip
        header[index + 3] = this.numberToSkip >> 24 & 0xff;
        header[index + 2] = this.numberToSkip >> 16 & 0xff;
        header[index + 1] = this.numberToSkip >> 8 & 0xff;
        header[index] = this.numberToSkip & 0xff;
        index = index + 4;
        // Write header information flags numberToReturn
        header[index + 3] = this.numberToReturn >> 24 & 0xff;
        header[index + 2] = this.numberToReturn >> 16 & 0xff;
        header[index + 1] = this.numberToReturn >> 8 & 0xff;
        header[index] = this.numberToReturn & 0xff;
        index = index + 4;
        // Return the buffers
        return buffers;
    }
}
exports.OpQueryRequest = OpQueryRequest;
/** @internal */ class OpReply {
    constructor(message, msgHeader, msgBody, opts){
        this.index = 0;
        this.sections = [];
        /** moreToCome is an OP_MSG only concept */ this.moreToCome = false;
        this.parsed = false;
        this.raw = message;
        this.data = msgBody;
        this.opts = opts ?? {
            useBigInt64: false,
            promoteLongs: true,
            promoteValues: true,
            promoteBuffers: false,
            bsonRegExp: false
        };
        // Read the message header
        this.length = msgHeader.length;
        this.requestId = msgHeader.requestId;
        this.responseTo = msgHeader.responseTo;
        this.opCode = msgHeader.opCode;
        this.fromCompressed = msgHeader.fromCompressed;
        // Flag values
        this.useBigInt64 = typeof this.opts.useBigInt64 === 'boolean' ? this.opts.useBigInt64 : false;
        this.promoteLongs = typeof this.opts.promoteLongs === 'boolean' ? this.opts.promoteLongs : true;
        this.promoteValues = typeof this.opts.promoteValues === 'boolean' ? this.opts.promoteValues : true;
        this.promoteBuffers = typeof this.opts.promoteBuffers === 'boolean' ? this.opts.promoteBuffers : false;
        this.bsonRegExp = typeof this.opts.bsonRegExp === 'boolean' ? this.opts.bsonRegExp : false;
    }
    isParsed() {
        return this.parsed;
    }
    parse() {
        // Don't parse again if not needed
        if (this.parsed) return this.sections[0];
        // Position within OP_REPLY at which documents start
        // (See https://www.mongodb.com/docs/manual/reference/mongodb-wire-protocol/#wire-op-reply)
        this.index = 20;
        // Read the message body
        this.responseFlags = this.data.readInt32LE(0);
        this.cursorId = new BSON.Long(this.data.readInt32LE(4), this.data.readInt32LE(8));
        this.startingFrom = this.data.readInt32LE(12);
        this.numberReturned = this.data.readInt32LE(16);
        if (this.numberReturned < 0 || this.numberReturned > 2 ** 32 - 1) {
            throw new RangeError(`OP_REPLY numberReturned is an invalid array length ${this.numberReturned}`);
        }
        this.cursorNotFound = (this.responseFlags & CURSOR_NOT_FOUND) !== 0;
        this.queryFailure = (this.responseFlags & QUERY_FAILURE) !== 0;
        this.shardConfigStale = (this.responseFlags & SHARD_CONFIG_STALE) !== 0;
        this.awaitCapable = (this.responseFlags & AWAIT_CAPABLE) !== 0;
        // Parse Body
        for(let i = 0; i < this.numberReturned; i++){
            const bsonSize = this.data[this.index] | this.data[this.index + 1] << 8 | this.data[this.index + 2] << 16 | this.data[this.index + 3] << 24;
            const section = this.data.subarray(this.index, this.index + bsonSize);
            this.sections.push(section);
            // Adjust the index
            this.index = this.index + bsonSize;
        }
        // Set parsed
        this.parsed = true;
        return this.sections[0];
    }
}
exports.OpReply = OpReply;
// Msg Flags
const OPTS_CHECKSUM_PRESENT = 1;
const OPTS_MORE_TO_COME = 2;
const OPTS_EXHAUST_ALLOWED = 1 << 16;
/** @internal */ class DocumentSequence {
    /**
     * Create a new document sequence for the provided field.
     * @param field - The field it will replace.
     */ constructor(field, documents){
        this.field = field;
        this.documents = [];
        this.chunks = [];
        this.serializedDocumentsLength = 0;
        // Document sequences starts with type 1 at the first byte.
        // Field strings must always be UTF-8.
        const buffer = Buffer.allocUnsafe(1 + 4 + this.field.length + 1);
        buffer[0] = 1;
        // Third part is the field name at offset 5 with trailing null byte.
        encodeUTF8Into(buffer, `${this.field}\0`, 5);
        this.chunks.push(buffer);
        this.header = buffer;
        if (documents) {
            for (const doc of documents){
                this.push(doc, BSON.serialize(doc));
            }
        }
    }
    /**
     * Push a document to the document sequence. Will serialize the document
     * as well and return the current serialized length of all documents.
     * @param document - The document to add.
     * @param buffer - The serialized document in raw BSON.
     * @returns The new total document sequence length.
     */ push(document, buffer) {
        this.serializedDocumentsLength += buffer.length;
        // Push the document.
        this.documents.push(document);
        // Push the document raw bson.
        this.chunks.push(buffer);
        // Write the new length.
        this.header?.writeInt32LE(4 + this.field.length + 1 + this.serializedDocumentsLength, 1);
        return this.serializedDocumentsLength + this.header.length;
    }
    /**
     * Get the fully serialized bytes for the document sequence section.
     * @returns The section bytes.
     */ toBin() {
        return Buffer.concat(this.chunks);
    }
}
exports.DocumentSequence = DocumentSequence;
/** @internal */ class OpMsgRequest {
    constructor(databaseName, command, options){
        this.databaseName = databaseName;
        this.command = command;
        this.options = options;
        // Basic options needed to be passed in
        if (command == null) throw new error_1.MongoInvalidArgumentError('Query document must be specified for query');
        // Basic options
        this.command.$db = databaseName;
        // Ensure empty options
        this.options = options ?? {};
        // Additional options
        this.requestId = options.requestId ? options.requestId : OpMsgRequest.getRequestId();
        // Serialization option
        this.serializeFunctions = typeof options.serializeFunctions === 'boolean' ? options.serializeFunctions : false;
        this.ignoreUndefined = typeof options.ignoreUndefined === 'boolean' ? options.ignoreUndefined : false;
        this.checkKeys = typeof options.checkKeys === 'boolean' ? options.checkKeys : false;
        this.maxBsonSize = options.maxBsonSize || 1024 * 1024 * 16;
        // flags
        this.checksumPresent = false;
        this.moreToCome = options.moreToCome ?? command.writeConcern?.w === 0;
        this.exhaustAllowed = typeof options.exhaustAllowed === 'boolean' ? options.exhaustAllowed : false;
    }
    toBin() {
        const buffers = [];
        let flags = 0;
        if (this.checksumPresent) {
            flags |= OPTS_CHECKSUM_PRESENT;
        }
        if (this.moreToCome) {
            flags |= OPTS_MORE_TO_COME;
        }
        if (this.exhaustAllowed) {
            flags |= OPTS_EXHAUST_ALLOWED;
        }
        const header = Buffer.alloc(4 * 4 + // Header
        4 // Flags
        );
        buffers.push(header);
        let totalLength = header.length;
        const command = this.command;
        totalLength += this.makeSections(buffers, command);
        header.writeInt32LE(totalLength, 0); // messageLength
        header.writeInt32LE(this.requestId, 4); // requestID
        header.writeInt32LE(0, 8); // responseTo
        header.writeInt32LE(constants_1.OP_MSG, 12); // opCode
        header.writeUInt32LE(flags, 16); // flags
        return buffers;
    }
    /**
     * Add the sections to the OP_MSG request's buffers and returns the length.
     */ makeSections(buffers, document) {
        const sequencesBuffer = this.extractDocumentSequences(document);
        const payloadTypeBuffer = Buffer.allocUnsafe(1);
        payloadTypeBuffer[0] = 0;
        const documentBuffer = this.serializeBson(document);
        // First section, type 0
        buffers.push(payloadTypeBuffer);
        buffers.push(documentBuffer);
        // Subsequent sections, type 1
        buffers.push(sequencesBuffer);
        return payloadTypeBuffer.length + documentBuffer.length + sequencesBuffer.length;
    }
    /**
     * Extracts the document sequences from the command document and returns
     * a buffer to be added as multiple sections after the initial type 0
     * section in the message.
     */ extractDocumentSequences(document) {
        // Pull out any field in the command document that's value is a document sequence.
        const chunks = [];
        for (const [key, value] of Object.entries(document)){
            if (value instanceof DocumentSequence) {
                chunks.push(value.toBin());
                // Why are we removing the field from the command? This is because it needs to be
                // removed in the OP_MSG request first section, and DocumentSequence is not a
                // BSON type and is specific to the MongoDB wire protocol so there's nothing
                // our BSON serializer can do about this. Since DocumentSequence is not exposed
                // in the public API and only used internally, we are never mutating an original
                // command provided by the user, just our own, and it's cheaper to delete from
                // our own command than copying it.
                delete document[key];
            }
        }
        if (chunks.length > 0) {
            return Buffer.concat(chunks);
        }
        // If we have no document sequences we return an empty buffer for nothing to add
        // to the payload.
        return Buffer.alloc(0);
    }
    serializeBson(document) {
        return BSON.serialize(document, {
            checkKeys: this.checkKeys,
            serializeFunctions: this.serializeFunctions,
            ignoreUndefined: this.ignoreUndefined
        });
    }
    static getRequestId() {
        _requestId = _requestId + 1 & 0x7fffffff;
        return _requestId;
    }
}
exports.OpMsgRequest = OpMsgRequest;
/** @internal */ class OpMsgResponse {
    constructor(message, msgHeader, msgBody, opts){
        this.index = 0;
        this.sections = [];
        this.parsed = false;
        this.raw = message;
        this.data = msgBody;
        this.opts = opts ?? {
            useBigInt64: false,
            promoteLongs: true,
            promoteValues: true,
            promoteBuffers: false,
            bsonRegExp: false
        };
        // Read the message header
        this.length = msgHeader.length;
        this.requestId = msgHeader.requestId;
        this.responseTo = msgHeader.responseTo;
        this.opCode = msgHeader.opCode;
        this.fromCompressed = msgHeader.fromCompressed;
        // Read response flags
        this.responseFlags = msgBody.readInt32LE(0);
        this.checksumPresent = (this.responseFlags & OPTS_CHECKSUM_PRESENT) !== 0;
        this.moreToCome = (this.responseFlags & OPTS_MORE_TO_COME) !== 0;
        this.exhaustAllowed = (this.responseFlags & OPTS_EXHAUST_ALLOWED) !== 0;
        this.useBigInt64 = typeof this.opts.useBigInt64 === 'boolean' ? this.opts.useBigInt64 : false;
        this.promoteLongs = typeof this.opts.promoteLongs === 'boolean' ? this.opts.promoteLongs : true;
        this.promoteValues = typeof this.opts.promoteValues === 'boolean' ? this.opts.promoteValues : true;
        this.promoteBuffers = typeof this.opts.promoteBuffers === 'boolean' ? this.opts.promoteBuffers : false;
        this.bsonRegExp = typeof this.opts.bsonRegExp === 'boolean' ? this.opts.bsonRegExp : false;
    }
    isParsed() {
        return this.parsed;
    }
    parse() {
        // Don't parse again if not needed
        if (this.parsed) return this.sections[0];
        this.index = 4;
        while(this.index < this.data.length){
            const payloadType = this.data.readUInt8(this.index++);
            if (payloadType === 0) {
                const bsonSize = this.data.readUInt32LE(this.index);
                const bin = this.data.subarray(this.index, this.index + bsonSize);
                this.sections.push(bin);
                this.index += bsonSize;
            } else if (payloadType === 1) {
                // It was decided that no driver makes use of payload type 1
                // TODO(NODE-3483): Replace with MongoDeprecationError
                throw new error_1.MongoRuntimeError('OP_MSG Payload Type 1 detected unsupported protocol');
            }
        }
        this.parsed = true;
        return this.sections[0];
    }
}
exports.OpMsgResponse = OpMsgResponse;
const MESSAGE_HEADER_SIZE = 16;
const COMPRESSION_DETAILS_SIZE = 9; // originalOpcode + uncompressedSize, compressorID
/**
 * @internal
 *
 * An OP_COMPRESSED request wraps either an OP_QUERY or OP_MSG message.
 */ class OpCompressedRequest {
    constructor(command, options){
        this.command = command;
        this.options = options;
    }
    // Return whether a command contains an uncompressible command term
    // Will return true if command contains no uncompressible command terms
    static canCompress(command) {
        const commandDoc = command instanceof OpMsgRequest ? command.command : command.query;
        const commandName = Object.keys(commandDoc)[0];
        return !compression_1.uncompressibleCommands.has(commandName);
    }
    async toBin() {
        const concatenatedOriginalCommandBuffer = Buffer.concat(this.command.toBin());
        // otherwise, compress the message
        const messageToBeCompressed = concatenatedOriginalCommandBuffer.slice(MESSAGE_HEADER_SIZE);
        // Extract information needed for OP_COMPRESSED from the uncompressed message
        const originalCommandOpCode = concatenatedOriginalCommandBuffer.readInt32LE(12);
        // Compress the message body
        const compressedMessage = await (0, compression_1.compress)(this.options, messageToBeCompressed);
        // Create the msgHeader of OP_COMPRESSED
        const msgHeader = Buffer.alloc(MESSAGE_HEADER_SIZE);
        msgHeader.writeInt32LE(MESSAGE_HEADER_SIZE + COMPRESSION_DETAILS_SIZE + compressedMessage.length, 0); // messageLength
        msgHeader.writeInt32LE(this.command.requestId, 4); // requestID
        msgHeader.writeInt32LE(0, 8); // responseTo (zero)
        msgHeader.writeInt32LE(constants_1.OP_COMPRESSED, 12); // opCode
        // Create the compression details of OP_COMPRESSED
        const compressionDetails = Buffer.alloc(COMPRESSION_DETAILS_SIZE);
        compressionDetails.writeInt32LE(originalCommandOpCode, 0); // originalOpcode
        compressionDetails.writeInt32LE(messageToBeCompressed.length, 4); // Size of the uncompressed compressedMessage, excluding the MsgHeader
        compressionDetails.writeUInt8(compression_1.Compressor[this.options.agreedCompressor], 8); // compressorID
        return [
            msgHeader,
            compressionDetails,
            compressedMessage
        ];
    }
}
exports.OpCompressedRequest = OpCompressedRequest; //# sourceMappingURL=commands.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.uncompressibleCommands = exports.Compressor = void 0;
exports.compress = compress;
exports.decompress = decompress;
exports.compressCommand = compressCommand;
exports.decompressResponse = decompressResponse;
const util_1 = require("util");
const zlib = require("zlib");
const constants_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/constants.js [app-route] (ecmascript)");
const deps_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/deps.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const commands_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/commands.js [app-route] (ecmascript)");
const constants_2 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [app-route] (ecmascript)");
/** @public */ exports.Compressor = Object.freeze({
    none: 0,
    snappy: 1,
    zlib: 2,
    zstd: 3
});
exports.uncompressibleCommands = new Set([
    constants_1.LEGACY_HELLO_COMMAND,
    'saslStart',
    'saslContinue',
    'getnonce',
    'authenticate',
    'createUser',
    'updateUser',
    'copydbSaslStart',
    'copydbgetnonce',
    'copydb'
]);
const ZSTD_COMPRESSION_LEVEL = 3;
const zlibInflate = (0, util_1.promisify)(zlib.inflate.bind(zlib));
const zlibDeflate = (0, util_1.promisify)(zlib.deflate.bind(zlib));
let zstd;
let Snappy = null;
function loadSnappy() {
    if (Snappy == null) {
        const snappyImport = (0, deps_1.getSnappy)();
        if ('kModuleError' in snappyImport) {
            throw snappyImport.kModuleError;
        }
        Snappy = snappyImport;
    }
    return Snappy;
}
// Facilitate compressing a message using an agreed compressor
async function compress(options, dataToBeCompressed) {
    const zlibOptions = {};
    switch(options.agreedCompressor){
        case 'snappy':
            {
                Snappy ??= loadSnappy();
                return await Snappy.compress(dataToBeCompressed);
            }
        case 'zstd':
            {
                loadZstd();
                if ('kModuleError' in zstd) {
                    throw zstd['kModuleError'];
                }
                return await zstd.compress(dataToBeCompressed, ZSTD_COMPRESSION_LEVEL);
            }
        case 'zlib':
            {
                if (options.zlibCompressionLevel) {
                    zlibOptions.level = options.zlibCompressionLevel;
                }
                return await zlibDeflate(dataToBeCompressed, zlibOptions);
            }
        default:
            {
                throw new error_1.MongoInvalidArgumentError(`Unknown compressor ${options.agreedCompressor} failed to compress`);
            }
    }
}
// Decompress a message using the given compressor
async function decompress(compressorID, compressedData) {
    if (compressorID !== exports.Compressor.snappy && compressorID !== exports.Compressor.zstd && compressorID !== exports.Compressor.zlib && compressorID !== exports.Compressor.none) {
        throw new error_1.MongoDecompressionError(`Server sent message compressed using an unsupported compressor. (Received compressor ID ${compressorID})`);
    }
    switch(compressorID){
        case exports.Compressor.snappy:
            {
                Snappy ??= loadSnappy();
                return await Snappy.uncompress(compressedData, {
                    asBuffer: true
                });
            }
        case exports.Compressor.zstd:
            {
                loadZstd();
                if ('kModuleError' in zstd) {
                    throw zstd['kModuleError'];
                }
                return await zstd.decompress(compressedData);
            }
        case exports.Compressor.zlib:
            {
                return await zlibInflate(compressedData);
            }
        default:
            {
                return compressedData;
            }
    }
}
/**
 * Load ZStandard if it is not already set.
 */ function loadZstd() {
    if (!zstd) {
        zstd = (0, deps_1.getZstdLibrary)();
    }
}
const MESSAGE_HEADER_SIZE = 16;
/**
 * @internal
 *
 * Compresses an OP_MSG or OP_QUERY message, if compression is configured.  This method
 * also serializes the command to BSON.
 */ async function compressCommand(command, description) {
    const finalCommand = description.agreedCompressor === 'none' || !commands_1.OpCompressedRequest.canCompress(command) ? command : new commands_1.OpCompressedRequest(command, {
        agreedCompressor: description.agreedCompressor ?? 'none',
        zlibCompressionLevel: description.zlibCompressionLevel ?? 0
    });
    const data = await finalCommand.toBin();
    return Buffer.concat(data);
}
/**
 * @internal
 *
 * Decompresses an OP_MSG or OP_QUERY response from the server, if compression is configured.
 *
 * This method does not parse the response's BSON.
 */ async function decompressResponse(message) {
    const messageHeader = {
        length: message.readInt32LE(0),
        requestId: message.readInt32LE(4),
        responseTo: message.readInt32LE(8),
        opCode: message.readInt32LE(12)
    };
    if (messageHeader.opCode !== constants_2.OP_COMPRESSED) {
        const ResponseType = messageHeader.opCode === constants_2.OP_MSG ? commands_1.OpMsgResponse : commands_1.OpReply;
        const messageBody = message.subarray(MESSAGE_HEADER_SIZE);
        return new ResponseType(message, messageHeader, messageBody);
    }
    const header = {
        ...messageHeader,
        fromCompressed: true,
        opCode: message.readInt32LE(MESSAGE_HEADER_SIZE),
        length: message.readInt32LE(MESSAGE_HEADER_SIZE + 4)
    };
    const compressorID = message[MESSAGE_HEADER_SIZE + 8];
    const compressedBuffer = message.slice(MESSAGE_HEADER_SIZE + 9);
    // recalculate based on wrapped opcode
    const ResponseType = header.opCode === constants_2.OP_MSG ? commands_1.OpMsgResponse : commands_1.OpReply;
    const messageBody = await decompress(compressorID, compressedBuffer);
    if (messageBody.length !== header.length) {
        throw new error_1.MongoDecompressionError('Message body and message header must be the same length');
    }
    return new ResponseType(message, header, messageBody);
} //# sourceMappingURL=compression.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/aws_temporary_credentials.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.LegacyAWSTemporaryCredentialProvider = exports.AWSSDKCredentialProvider = exports.AWSTemporaryCredentialProvider = void 0;
const deps_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/deps.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const AWS_RELATIVE_URI = 'http://169.254.170.2';
const AWS_EC2_URI = 'http://169.254.169.254';
const AWS_EC2_PATH = '/latest/meta-data/iam/security-credentials';
/**
 * @internal
 *
 * Fetches temporary AWS credentials.
 */ class AWSTemporaryCredentialProvider {
    static get awsSDK() {
        AWSTemporaryCredentialProvider._awsSDK ??= (0, deps_1.getAwsCredentialProvider)();
        return AWSTemporaryCredentialProvider._awsSDK;
    }
    static get isAWSSDKInstalled() {
        return !('kModuleError' in AWSTemporaryCredentialProvider.awsSDK);
    }
}
exports.AWSTemporaryCredentialProvider = AWSTemporaryCredentialProvider;
/** @internal */ class AWSSDKCredentialProvider extends AWSTemporaryCredentialProvider {
    /**
     * Create the SDK credentials provider.
     * @param credentialsProvider - The credentials provider.
     */ constructor(credentialsProvider){
        super();
        if (credentialsProvider) {
            this._provider = credentialsProvider;
        }
    }
    /**
     * The AWS SDK caches credentials automatically and handles refresh when the credentials have expired.
     * To ensure this occurs, we need to cache the `provider` returned by the AWS sdk and re-use it when fetching credentials.
     */ get provider() {
        if ('kModuleError' in AWSTemporaryCredentialProvider.awsSDK) {
            throw AWSTemporaryCredentialProvider.awsSDK.kModuleError;
        }
        if (this._provider) {
            return this._provider;
        }
        let { AWS_STS_REGIONAL_ENDPOINTS = '', AWS_REGION = '' } = process.env;
        AWS_STS_REGIONAL_ENDPOINTS = AWS_STS_REGIONAL_ENDPOINTS.toLowerCase();
        AWS_REGION = AWS_REGION.toLowerCase();
        /** The option setting should work only for users who have explicit settings in their environment, the driver should not encode "defaults" */ const awsRegionSettingsExist = AWS_REGION.length !== 0 && AWS_STS_REGIONAL_ENDPOINTS.length !== 0;
        /**
         * The following regions use the global AWS STS endpoint, sts.amazonaws.com, by default
         * https://docs.aws.amazon.com/sdkref/latest/guide/feature-sts-regionalized-endpoints.html
         */ const LEGACY_REGIONS = new Set([
            'ap-northeast-1',
            'ap-south-1',
            'ap-southeast-1',
            'ap-southeast-2',
            'aws-global',
            'ca-central-1',
            'eu-central-1',
            'eu-north-1',
            'eu-west-1',
            'eu-west-2',
            'eu-west-3',
            'sa-east-1',
            'us-east-1',
            'us-east-2',
            'us-west-1',
            'us-west-2'
        ]);
        /**
         * If AWS_STS_REGIONAL_ENDPOINTS is set to regional, users are opting into the new behavior of respecting the region settings
         *
         * If AWS_STS_REGIONAL_ENDPOINTS is set to legacy, then "old" regions need to keep using the global setting.
         * Technically the SDK gets this wrong, it reaches out to 'sts.us-east-1.amazonaws.com' when it should be 'sts.amazonaws.com'.
         * That is not our bug to fix here. We leave that up to the SDK.
         */ const useRegionalSts = AWS_STS_REGIONAL_ENDPOINTS === 'regional' || AWS_STS_REGIONAL_ENDPOINTS === 'legacy' && !LEGACY_REGIONS.has(AWS_REGION);
        this._provider = awsRegionSettingsExist && useRegionalSts ? AWSTemporaryCredentialProvider.awsSDK.fromNodeProviderChain({
            clientConfig: {
                region: AWS_REGION
            }
        }) : AWSTemporaryCredentialProvider.awsSDK.fromNodeProviderChain();
        return this._provider;
    }
    async getCredentials() {
        /*
         * Creates a credential provider that will attempt to find credentials from the
         * following sources (listed in order of precedence):
         *
         * - Environment variables exposed via process.env
         * - SSO credentials from token cache
         * - Web identity token credentials
         * - Shared credentials and config ini files
         * - The EC2/ECS Instance Metadata Service
         */ try {
            const creds = await this.provider();
            return {
                AccessKeyId: creds.accessKeyId,
                SecretAccessKey: creds.secretAccessKey,
                Token: creds.sessionToken,
                Expiration: creds.expiration
            };
        } catch (error) {
            throw new error_1.MongoAWSError(error.message, {
                cause: error
            });
        }
    }
}
exports.AWSSDKCredentialProvider = AWSSDKCredentialProvider;
/**
 * @internal
 * Fetches credentials manually (without the AWS SDK), as outlined in the [Obtaining Credentials](https://github.com/mongodb/specifications/blob/master/source/auth/auth.md#obtaining-credentials)
 * section of the Auth spec.
 */ class LegacyAWSTemporaryCredentialProvider extends AWSTemporaryCredentialProvider {
    async getCredentials() {
        // If the environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI
        // is set then drivers MUST assume that it was set by an AWS ECS agent
        if (process.env.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI) {
            return await (0, utils_1.request)(`${AWS_RELATIVE_URI}${process.env.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI}`);
        }
        // Otherwise assume we are on an EC2 instance
        // get a token
        const token = await (0, utils_1.request)(`${AWS_EC2_URI}/latest/api/token`, {
            method: 'PUT',
            json: false,
            headers: {
                'X-aws-ec2-metadata-token-ttl-seconds': 30
            }
        });
        // get role name
        const roleName = await (0, utils_1.request)(`${AWS_EC2_URI}/${AWS_EC2_PATH}`, {
            json: false,
            headers: {
                'X-aws-ec2-metadata-token': token
            }
        });
        // get temp credentials
        const creds = await (0, utils_1.request)(`${AWS_EC2_URI}/${AWS_EC2_PATH}/${roleName}`, {
            headers: {
                'X-aws-ec2-metadata-token': token
            }
        });
        return creds;
    }
}
exports.LegacyAWSTemporaryCredentialProvider = LegacyAWSTemporaryCredentialProvider; //# sourceMappingURL=aws_temporary_credentials.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/metrics.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ConnectionPoolMetrics = void 0;
/** @internal */ class ConnectionPoolMetrics {
    constructor(){
        this.txnConnections = 0;
        this.cursorConnections = 0;
        this.otherConnections = 0;
    }
    /**
     * Mark a connection as pinned for a specific operation.
     */ markPinned(pinType) {
        if (pinType === ConnectionPoolMetrics.TXN) {
            this.txnConnections += 1;
        } else if (pinType === ConnectionPoolMetrics.CURSOR) {
            this.cursorConnections += 1;
        } else {
            this.otherConnections += 1;
        }
    }
    /**
     * Unmark a connection as pinned for an operation.
     */ markUnpinned(pinType) {
        if (pinType === ConnectionPoolMetrics.TXN) {
            this.txnConnections -= 1;
        } else if (pinType === ConnectionPoolMetrics.CURSOR) {
            this.cursorConnections -= 1;
        } else {
            this.otherConnections -= 1;
        }
    }
    /**
     * Return information about the cmap metrics as a string.
     */ info(maxPoolSize) {
        return 'Timed out while checking out a connection from connection pool: ' + `maxPoolSize: ${maxPoolSize}, ` + `connections in use by cursors: ${this.cursorConnections}, ` + `connections in use by transactions: ${this.txnConnections}, ` + `connections in use by other operations: ${this.otherConnections}`;
    }
    /**
     * Reset the metrics to the initial values.
     */ reset() {
        this.txnConnections = 0;
        this.cursorConnections = 0;
        this.otherConnections = 0;
    }
}
exports.ConnectionPoolMetrics = ConnectionPoolMetrics;
ConnectionPoolMetrics.TXN = 'txn';
ConnectionPoolMetrics.CURSOR = 'cursor';
ConnectionPoolMetrics.OTHER = 'other'; //# sourceMappingURL=metrics.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/shared.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.getReadPreference = getReadPreference;
exports.isSharded = isSharded;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const read_preference_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/read_preference.js [app-route] (ecmascript)");
const common_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/sdam/common.js [app-route] (ecmascript)");
const topology_description_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/sdam/topology_description.js [app-route] (ecmascript)");
function getReadPreference(options) {
    // Default to command version of the readPreference.
    let readPreference = options?.readPreference ?? read_preference_1.ReadPreference.primary;
    if (typeof readPreference === 'string') {
        readPreference = read_preference_1.ReadPreference.fromString(readPreference);
    }
    if (!(readPreference instanceof read_preference_1.ReadPreference)) {
        throw new error_1.MongoInvalidArgumentError('Option "readPreference" must be a ReadPreference instance');
    }
    return readPreference;
}
function isSharded(topologyOrServer) {
    if (topologyOrServer == null) {
        return false;
    }
    if (topologyOrServer.description && topologyOrServer.description.type === common_1.ServerType.Mongos) {
        return true;
    }
    // NOTE: This is incredibly inefficient, and should be removed once command construction
    // happens based on `Server` not `Topology`.
    if (topologyOrServer.description && topologyOrServer.description instanceof topology_description_1.TopologyDescription) {
        const servers = Array.from(topologyOrServer.description.servers.values());
        return servers.some((server)=>server.type === common_1.ServerType.Mongos);
    }
    return false;
} //# sourceMappingURL=shared.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/command_monitoring_events.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.SENSITIVE_COMMANDS = exports.CommandFailedEvent = exports.CommandSucceededEvent = exports.CommandStartedEvent = void 0;
const constants_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/constants.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const commands_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/commands.js [app-route] (ecmascript)");
/**
 * An event indicating the start of a given command
 * @public
 * @category Event
 */ class CommandStartedEvent {
    /**
     * Create a started event
     *
     * @internal
     * @param pool - the pool that originated the command
     * @param command - the command
     */ constructor(connection, command, serverConnectionId){
        /** @internal */ this.name = constants_1.COMMAND_STARTED;
        const cmd = extractCommand(command);
        const commandName = extractCommandName(cmd);
        const { address, connectionId, serviceId } = extractConnectionDetails(connection);
        // TODO: remove in major revision, this is not spec behavior
        if (exports.SENSITIVE_COMMANDS.has(commandName)) {
            this.commandObj = {};
            this.commandObj[commandName] = true;
        }
        this.address = address;
        this.connectionId = connectionId;
        this.serviceId = serviceId;
        this.requestId = command.requestId;
        this.databaseName = command.databaseName;
        this.commandName = commandName;
        this.command = maybeRedact(commandName, cmd, cmd);
        this.serverConnectionId = serverConnectionId;
    }
    /* @internal */ get hasServiceId() {
        return !!this.serviceId;
    }
}
exports.CommandStartedEvent = CommandStartedEvent;
/**
 * An event indicating the success of a given command
 * @public
 * @category Event
 */ class CommandSucceededEvent {
    /**
     * Create a succeeded event
     *
     * @internal
     * @param pool - the pool that originated the command
     * @param command - the command
     * @param reply - the reply for this command from the server
     * @param started - a high resolution tuple timestamp of when the command was first sent, to calculate duration
     */ constructor(connection, command, reply, started, serverConnectionId){
        /** @internal */ this.name = constants_1.COMMAND_SUCCEEDED;
        const cmd = extractCommand(command);
        const commandName = extractCommandName(cmd);
        const { address, connectionId, serviceId } = extractConnectionDetails(connection);
        this.address = address;
        this.connectionId = connectionId;
        this.serviceId = serviceId;
        this.requestId = command.requestId;
        this.commandName = commandName;
        this.duration = (0, utils_1.calculateDurationInMs)(started);
        this.reply = maybeRedact(commandName, cmd, extractReply(reply));
        this.serverConnectionId = serverConnectionId;
    }
    /* @internal */ get hasServiceId() {
        return !!this.serviceId;
    }
}
exports.CommandSucceededEvent = CommandSucceededEvent;
/**
 * An event indicating the failure of a given command
 * @public
 * @category Event
 */ class CommandFailedEvent {
    /**
     * Create a failure event
     *
     * @internal
     * @param pool - the pool that originated the command
     * @param command - the command
     * @param error - the generated error or a server error response
     * @param started - a high resolution tuple timestamp of when the command was first sent, to calculate duration
     */ constructor(connection, command, error, started, serverConnectionId){
        /** @internal */ this.name = constants_1.COMMAND_FAILED;
        const cmd = extractCommand(command);
        const commandName = extractCommandName(cmd);
        const { address, connectionId, serviceId } = extractConnectionDetails(connection);
        this.address = address;
        this.connectionId = connectionId;
        this.serviceId = serviceId;
        this.requestId = command.requestId;
        this.commandName = commandName;
        this.duration = (0, utils_1.calculateDurationInMs)(started);
        this.failure = maybeRedact(commandName, cmd, error);
        this.serverConnectionId = serverConnectionId;
    }
    /* @internal */ get hasServiceId() {
        return !!this.serviceId;
    }
}
exports.CommandFailedEvent = CommandFailedEvent;
/**
 * Commands that we want to redact because of the sensitive nature of their contents
 * @internal
 */ exports.SENSITIVE_COMMANDS = new Set([
    'authenticate',
    'saslStart',
    'saslContinue',
    'getnonce',
    'createUser',
    'updateUser',
    'copydbgetnonce',
    'copydbsaslstart',
    'copydb'
]);
const HELLO_COMMANDS = new Set([
    'hello',
    constants_1.LEGACY_HELLO_COMMAND,
    constants_1.LEGACY_HELLO_COMMAND_CAMEL_CASE
]);
// helper methods
const extractCommandName = (commandDoc)=>Object.keys(commandDoc)[0];
const collectionName = (command)=>command.ns.split('.')[1];
const maybeRedact = (commandName, commandDoc, result)=>exports.SENSITIVE_COMMANDS.has(commandName) || HELLO_COMMANDS.has(commandName) && commandDoc.speculativeAuthenticate ? {} : result;
const LEGACY_FIND_QUERY_MAP = {
    $query: 'filter',
    $orderby: 'sort',
    $hint: 'hint',
    $comment: 'comment',
    $maxScan: 'maxScan',
    $max: 'max',
    $min: 'min',
    $returnKey: 'returnKey',
    $showDiskLoc: 'showRecordId',
    $maxTimeMS: 'maxTimeMS',
    $snapshot: 'snapshot'
};
const LEGACY_FIND_OPTIONS_MAP = {
    numberToSkip: 'skip',
    numberToReturn: 'batchSize',
    returnFieldSelector: 'projection'
};
/** Extract the actual command from the query, possibly up-converting if it's a legacy format */ function extractCommand(command) {
    if (command instanceof commands_1.OpMsgRequest) {
        const cmd = {
            ...command.command
        };
        // For OP_MSG with payload type 1 we need to pull the documents
        // array out of the document sequence for monitoring.
        if (cmd.ops instanceof commands_1.DocumentSequence) {
            cmd.ops = cmd.ops.documents;
        }
        if (cmd.nsInfo instanceof commands_1.DocumentSequence) {
            cmd.nsInfo = cmd.nsInfo.documents;
        }
        return cmd;
    }
    if (command.query?.$query) {
        let result;
        if (command.ns === 'admin.$cmd') {
            // up-convert legacy command
            result = Object.assign({}, command.query.$query);
        } else {
            // up-convert legacy find command
            result = {
                find: collectionName(command)
            };
            Object.keys(LEGACY_FIND_QUERY_MAP).forEach((key)=>{
                if (command.query[key] != null) {
                    result[LEGACY_FIND_QUERY_MAP[key]] = {
                        ...command.query[key]
                    };
                }
            });
        }
        Object.keys(LEGACY_FIND_OPTIONS_MAP).forEach((key)=>{
            const legacyKey = key;
            if (command[legacyKey] != null) {
                result[LEGACY_FIND_OPTIONS_MAP[legacyKey]] = command[legacyKey];
            }
        });
        return result;
    }
    let clonedQuery = {};
    const clonedCommand = {
        ...command
    };
    if (command.query) {
        clonedQuery = {
            ...command.query
        };
        clonedCommand.query = clonedQuery;
    }
    return command.query ? clonedQuery : clonedCommand;
}
function extractReply(reply) {
    if (!reply) {
        return reply;
    }
    return reply.result ? reply.result : reply;
}
function extractConnectionDetails(connection) {
    let connectionId;
    if ('id' in connection) {
        connectionId = connection.id;
    }
    return {
        address: connection.address,
        serviceId: connection.serviceId,
        connectionId
    };
} //# sourceMappingURL=command_monitoring_events.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/stream_description.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.StreamDescription = void 0;
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const common_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/sdam/common.js [app-route] (ecmascript)");
const server_description_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/sdam/server_description.js [app-route] (ecmascript)");
const RESPONSE_FIELDS = [
    'minWireVersion',
    'maxWireVersion',
    'maxBsonObjectSize',
    'maxMessageSizeBytes',
    'maxWriteBatchSize',
    'logicalSessionTimeoutMinutes'
];
/** @public */ class StreamDescription {
    constructor(address, options){
        this.hello = null;
        this.address = address;
        this.type = common_1.ServerType.Unknown;
        this.minWireVersion = undefined;
        this.maxWireVersion = undefined;
        this.maxBsonObjectSize = 16777216;
        this.maxMessageSizeBytes = 48000000;
        this.maxWriteBatchSize = 100000;
        this.logicalSessionTimeoutMinutes = options?.logicalSessionTimeoutMinutes;
        this.loadBalanced = !!options?.loadBalanced;
        this.compressors = options && options.compressors && Array.isArray(options.compressors) ? options.compressors : [];
        this.serverConnectionId = null;
    }
    receiveResponse(response) {
        if (response == null) {
            return;
        }
        this.hello = response;
        this.type = (0, server_description_1.parseServerType)(response);
        if ('connectionId' in response) {
            this.serverConnectionId = this.parseServerConnectionID(response.connectionId);
        } else {
            this.serverConnectionId = null;
        }
        for (const field of RESPONSE_FIELDS){
            if (response[field] != null) {
                this[field] = response[field];
            }
            // testing case
            if ('__nodejs_mock_server__' in response) {
                this.__nodejs_mock_server__ = response['__nodejs_mock_server__'];
            }
        }
        if (response.compression) {
            this.compressor = this.compressors.filter((c)=>response.compression?.includes(c))[0];
        }
    }
    /* @internal */ parseServerConnectionID(serverConnectionId) {
        // Connection ids are always integral, so it's safe to coerce doubles as well as
        // any integral type.
        return bson_1.Long.isLong(serverConnectionId) ? serverConnectionId.toBigInt() : BigInt(serverConnectionId);
    }
}
exports.StreamDescription = StreamDescription; //# sourceMappingURL=stream_description.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/on_data.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.onData = onData;
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
/**
 * onData is adapted from Node.js' events.on helper
 * https://nodejs.org/api/events.html#eventsonemitter-eventname-options
 *
 * Returns an AsyncIterator that iterates each 'data' event emitted from emitter.
 * It will reject upon an error event.
 */ function onData(emitter, { timeoutContext, signal }) {
    signal?.throwIfAborted();
    // Setup pending events and pending promise lists
    /**
     * When the caller has not yet called .next(), we store the
     * value from the event in this list. Next time they call .next()
     * we pull the first value out of this list and resolve a promise with it.
     */ const unconsumedEvents = new utils_1.List();
    /**
     * When there has not yet been an event, a new promise will be created
     * and implicitly stored in this list. When an event occurs we take the first
     * promise in this list and resolve it.
     */ const unconsumedPromises = new utils_1.List();
    /**
     * Stored an error created by an error event.
     * This error will turn into a rejection for the subsequent .next() call
     */ let error = null;
    /** Set to true only after event listeners have been removed. */ let finished = false;
    const iterator = {
        next () {
            // First, we consume all unread events
            const value = unconsumedEvents.shift();
            if (value != null) {
                return Promise.resolve({
                    value,
                    done: false
                });
            }
            // Then we error, if an error happened
            // This happens one time if at all, because after 'error'
            // we stop listening
            if (error != null) {
                const p = Promise.reject(error);
                // Only the first element errors
                error = null;
                return p;
            }
            // If the iterator is finished, resolve to done
            if (finished) return closeHandler();
            // Wait until an event happens
            const { promise, resolve, reject } = (0, utils_1.promiseWithResolvers)();
            unconsumedPromises.push({
                resolve,
                reject
            });
            return promise;
        },
        return () {
            return closeHandler();
        },
        throw (err) {
            errorHandler(err);
            return Promise.resolve({
                value: undefined,
                done: true
            });
        },
        [Symbol.asyncIterator] () {
            return this;
        }
    };
    // Adding event handlers
    emitter.on('data', eventHandler);
    emitter.on('error', errorHandler);
    const abortListener = (0, utils_1.addAbortListener)(signal, function() {
        errorHandler(this.reason);
    });
    const timeoutForSocketRead = timeoutContext?.timeoutForSocketRead;
    timeoutForSocketRead?.throwIfExpired();
    timeoutForSocketRead?.then(undefined, errorHandler);
    return iterator;
    function eventHandler(value) {
        const promise = unconsumedPromises.shift();
        if (promise != null) promise.resolve({
            value,
            done: false
        });
        else unconsumedEvents.push(value);
    }
    function errorHandler(err) {
        const promise = unconsumedPromises.shift();
        if (promise != null) promise.reject(err);
        else error = err;
        void closeHandler();
    }
    function closeHandler() {
        // Adding event handlers
        emitter.off('data', eventHandler);
        emitter.off('error', errorHandler);
        abortListener?.[utils_1.kDispose]();
        finished = true;
        timeoutForSocketRead?.clear();
        const doneResult = {
            value: undefined,
            done: finished
        };
        for (const promise of unconsumedPromises){
            promise.resolve(doneResult);
        }
        return Promise.resolve(doneResult);
    }
} //# sourceMappingURL=on_data.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/connection.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CryptoConnection = exports.SizedMessageTransform = exports.Connection = void 0;
exports.hasSessionSupport = hasSessionSupport;
const stream_1 = require("stream");
const timers_1 = require("timers");
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const constants_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/constants.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const mongo_logger_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/mongo_logger.js [app-route] (ecmascript)");
const mongo_types_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/mongo_types.js [app-route] (ecmascript)");
const read_preference_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/read_preference.js [app-route] (ecmascript)");
const common_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/sdam/common.js [app-route] (ecmascript)");
const sessions_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/sessions.js [app-route] (ecmascript)");
const timeout_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/timeout.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const command_monitoring_events_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/command_monitoring_events.js [app-route] (ecmascript)");
const commands_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/commands.js [app-route] (ecmascript)");
const stream_description_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/stream_description.js [app-route] (ecmascript)");
const compression_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [app-route] (ecmascript)");
const on_data_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/on_data.js [app-route] (ecmascript)");
const responses_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [app-route] (ecmascript)");
const shared_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/shared.js [app-route] (ecmascript)");
/** @internal */ function hasSessionSupport(conn) {
    const description = conn.description;
    return description.logicalSessionTimeoutMinutes != null;
}
function streamIdentifier(stream, options) {
    if (options.proxyHost) {
        // If proxy options are specified, the properties of `stream` itself
        // will not accurately reflect what endpoint this is connected to.
        return options.hostAddress.toString();
    }
    const { remoteAddress, remotePort } = stream;
    if (typeof remoteAddress === 'string' && typeof remotePort === 'number') {
        return utils_1.HostAddress.fromHostPort(remoteAddress, remotePort).toString();
    }
    return (0, utils_1.uuidV4)().toString('hex');
}
/** @internal */ class Connection extends mongo_types_1.TypedEventEmitter {
    constructor(stream, options){
        super();
        this.lastHelloMS = -1;
        this.helloOk = false;
        this.delayedTimeoutId = null;
        /** Indicates that the connection (including underlying TCP socket) has been closed. */ this.closed = false;
        this.clusterTime = null;
        this.error = null;
        this.dataEvents = null;
        this.on('error', utils_1.noop);
        this.socket = stream;
        this.id = options.id;
        this.address = streamIdentifier(stream, options);
        this.socketTimeoutMS = options.socketTimeoutMS ?? 0;
        this.monitorCommands = options.monitorCommands;
        this.serverApi = options.serverApi;
        this.mongoLogger = options.mongoLogger;
        this.established = false;
        this.description = new stream_description_1.StreamDescription(this.address, options);
        this.generation = options.generation;
        this.lastUseTime = (0, utils_1.now)();
        this.messageStream = this.socket.on('error', this.onSocketError.bind(this)).pipe(new SizedMessageTransform({
            connection: this
        })).on('error', this.onTransformError.bind(this));
        this.socket.on('close', this.onClose.bind(this));
        this.socket.on('timeout', this.onTimeout.bind(this));
        this.messageStream.pause();
    }
    get hello() {
        return this.description.hello;
    }
    // the `connect` method stores the result of the handshake hello on the connection
    set hello(response) {
        this.description.receiveResponse(response);
        Object.freeze(this.description);
    }
    get serviceId() {
        return this.hello?.serviceId;
    }
    get loadBalanced() {
        return this.description.loadBalanced;
    }
    get idleTime() {
        return (0, utils_1.calculateDurationInMs)(this.lastUseTime);
    }
    get hasSessionSupport() {
        return this.description.logicalSessionTimeoutMinutes != null;
    }
    get supportsOpMsg() {
        return this.description != null && (0, utils_1.maxWireVersion)(this) >= 6 && !this.description.__nodejs_mock_server__;
    }
    get shouldEmitAndLogCommand() {
        return (this.monitorCommands || this.established && !this.authContext?.reauthenticating && this.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.COMMAND, mongo_logger_1.SeverityLevel.DEBUG)) ?? false;
    }
    markAvailable() {
        this.lastUseTime = (0, utils_1.now)();
    }
    onSocketError(cause) {
        this.onError(new error_1.MongoNetworkError(cause.message, {
            cause
        }));
    }
    onTransformError(error) {
        this.onError(error);
    }
    onError(error) {
        this.cleanup(error);
    }
    onClose() {
        const message = `connection ${this.id} to ${this.address} closed`;
        this.cleanup(new error_1.MongoNetworkError(message));
    }
    onTimeout() {
        this.delayedTimeoutId = (0, timers_1.setTimeout)(()=>{
            const message = `connection ${this.id} to ${this.address} timed out`;
            const beforeHandshake = this.hello == null;
            this.cleanup(new error_1.MongoNetworkTimeoutError(message, {
                beforeHandshake
            }));
        }, 1).unref(); // No need for this timer to hold the event loop open
    }
    destroy() {
        if (this.closed) {
            return;
        }
        // load balanced mode requires that these listeners remain on the connection
        // after cleanup on timeouts, errors or close so we remove them before calling
        // cleanup.
        this.removeAllListeners(Connection.PINNED);
        this.removeAllListeners(Connection.UNPINNED);
        const message = `connection ${this.id} to ${this.address} closed`;
        this.cleanup(new error_1.MongoNetworkError(message));
    }
    /**
     * A method that cleans up the connection.  When `force` is true, this method
     * forcibly destroys the socket.
     *
     * If an error is provided, any in-flight operations will be closed with the error.
     *
     * This method does nothing if the connection is already closed.
     */ cleanup(error) {
        if (this.closed) {
            return;
        }
        this.socket.destroy();
        this.error = error;
        this.dataEvents?.throw(error).then(undefined, utils_1.squashError);
        this.closed = true;
        this.emit(Connection.CLOSE);
    }
    prepareCommand(db, command, options) {
        let cmd = {
            ...command
        };
        const readPreference = (0, shared_1.getReadPreference)(options);
        const session = options?.session;
        let clusterTime = this.clusterTime;
        if (this.serverApi) {
            const { version, strict, deprecationErrors } = this.serverApi;
            cmd.apiVersion = version;
            if (strict != null) cmd.apiStrict = strict;
            if (deprecationErrors != null) cmd.apiDeprecationErrors = deprecationErrors;
        }
        if (this.hasSessionSupport && session) {
            if (session.clusterTime && clusterTime && session.clusterTime.clusterTime.greaterThan(clusterTime.clusterTime)) {
                clusterTime = session.clusterTime;
            }
            const sessionError = (0, sessions_1.applySession)(session, cmd, options);
            if (sessionError) throw sessionError;
        } else if (session?.explicit) {
            throw new error_1.MongoCompatibilityError('Current topology does not support sessions');
        }
        // if we have a known cluster time, gossip it
        if (clusterTime) {
            cmd.$clusterTime = clusterTime;
        }
        // For standalone, drivers MUST NOT set $readPreference.
        if (this.description.type !== common_1.ServerType.Standalone) {
            if (!(0, shared_1.isSharded)(this) && !this.description.loadBalanced && this.supportsOpMsg && options.directConnection === true && readPreference?.mode === 'primary') {
                // For mongos and load balancers with 'primary' mode, drivers MUST NOT set $readPreference.
                // For all other types with a direct connection, if the read preference is 'primary'
                // (driver sets 'primary' as default if no read preference is configured),
                // the $readPreference MUST be set to 'primaryPreferred'
                // to ensure that any server type can handle the request.
                cmd.$readPreference = read_preference_1.ReadPreference.primaryPreferred.toJSON();
            } else if ((0, shared_1.isSharded)(this) && !this.supportsOpMsg && readPreference?.mode !== 'primary') {
                // When sending a read operation via OP_QUERY and the $readPreference modifier,
                // the query MUST be provided using the $query modifier.
                cmd = {
                    $query: cmd,
                    $readPreference: readPreference.toJSON()
                };
            } else if (readPreference?.mode !== 'primary') {
                // For mode 'primary', drivers MUST NOT set $readPreference.
                // For all other read preference modes (i.e. 'secondary', 'primaryPreferred', ...),
                // drivers MUST set $readPreference
                cmd.$readPreference = readPreference.toJSON();
            }
        }
        const commandOptions = {
            numberToSkip: 0,
            numberToReturn: -1,
            checkKeys: false,
            // This value is not overridable
            secondaryOk: readPreference.secondaryOk(),
            ...options
        };
        options.timeoutContext?.addMaxTimeMSToCommand(cmd, options);
        const message = this.supportsOpMsg ? new commands_1.OpMsgRequest(db, cmd, commandOptions) : new commands_1.OpQueryRequest(db, cmd, commandOptions);
        return message;
    }
    async *sendWire(message, options, responseType) {
        this.throwIfAborted();
        const timeout = options.socketTimeoutMS ?? options?.timeoutContext?.getSocketTimeoutMS() ?? this.socketTimeoutMS;
        this.socket.setTimeout(timeout);
        try {
            await this.writeCommand(message, {
                agreedCompressor: this.description.compressor ?? 'none',
                zlibCompressionLevel: this.description.zlibCompressionLevel,
                timeoutContext: options.timeoutContext,
                signal: options.signal
            });
            if (options.noResponse || message.moreToCome) {
                yield responses_1.MongoDBResponse.empty;
                return;
            }
            this.throwIfAborted();
            if (options.timeoutContext?.csotEnabled() && options.timeoutContext.minRoundTripTime != null && options.timeoutContext.remainingTimeMS < options.timeoutContext.minRoundTripTime) {
                throw new error_1.MongoOperationTimeoutError('Server roundtrip time is greater than the time remaining');
            }
            for await (const response of this.readMany(options)){
                this.socket.setTimeout(0);
                const bson = response.parse();
                const document = (responseType ?? responses_1.MongoDBResponse).make(bson);
                yield document;
                this.throwIfAborted();
                this.socket.setTimeout(timeout);
            }
        } finally{
            this.socket.setTimeout(0);
        }
    }
    async *sendCommand(ns, command, options, responseType) {
        options?.signal?.throwIfAborted();
        const message = this.prepareCommand(ns.db, command, options);
        let started = 0;
        if (this.shouldEmitAndLogCommand) {
            started = (0, utils_1.now)();
            this.emitAndLogCommand(this.monitorCommands, Connection.COMMAND_STARTED, message.databaseName, this.established, new command_monitoring_events_1.CommandStartedEvent(this, message, this.description.serverConnectionId));
        }
        // If `documentsReturnedIn` not set or raw is not enabled, use input bson options
        // Otherwise, support raw flag. Raw only works for cursors that hardcode firstBatch/nextBatch fields
        const bsonOptions = options.documentsReturnedIn == null || !options.raw ? options : {
            ...options,
            raw: false,
            fieldsAsRaw: {
                [options.documentsReturnedIn]: true
            }
        };
        /** MongoDBResponse instance or subclass */ let document = undefined;
        /** Cached result of a toObject call */ let object = undefined;
        try {
            this.throwIfAborted();
            for await (document of this.sendWire(message, options, responseType)){
                object = undefined;
                if (options.session != null) {
                    (0, sessions_1.updateSessionFromResponse)(options.session, document);
                }
                if (document.$clusterTime) {
                    this.clusterTime = document.$clusterTime;
                    this.emit(Connection.CLUSTER_TIME_RECEIVED, document.$clusterTime);
                }
                if (document.ok === 0) {
                    if (options.timeoutContext?.csotEnabled() && document.isMaxTimeExpiredError) {
                        throw new error_1.MongoOperationTimeoutError('Server reported a timeout error', {
                            cause: new error_1.MongoServerError(object ??= document.toObject(bsonOptions))
                        });
                    }
                    throw new error_1.MongoServerError(object ??= document.toObject(bsonOptions));
                }
                if (this.shouldEmitAndLogCommand) {
                    this.emitAndLogCommand(this.monitorCommands, Connection.COMMAND_SUCCEEDED, message.databaseName, this.established, new command_monitoring_events_1.CommandSucceededEvent(this, message, options.noResponse ? undefined : message.moreToCome ? {
                        ok: 1
                    } : object ??= document.toObject(bsonOptions), started, this.description.serverConnectionId));
                }
                if (responseType == null) {
                    yield object ??= document.toObject(bsonOptions);
                } else {
                    yield document;
                }
                this.throwIfAborted();
            }
        } catch (error) {
            if (this.shouldEmitAndLogCommand) {
                this.emitAndLogCommand(this.monitorCommands, Connection.COMMAND_FAILED, message.databaseName, this.established, new command_monitoring_events_1.CommandFailedEvent(this, message, error, started, this.description.serverConnectionId));
            }
            throw error;
        }
    }
    async command(ns, command, options = {}, responseType) {
        this.throwIfAborted();
        options.signal?.throwIfAborted();
        for await (const document of this.sendCommand(ns, command, options, responseType)){
            if (options.timeoutContext?.csotEnabled()) {
                if (responses_1.MongoDBResponse.is(document)) {
                    if (document.isMaxTimeExpiredError) {
                        throw new error_1.MongoOperationTimeoutError('Server reported a timeout error', {
                            cause: new error_1.MongoServerError(document.toObject())
                        });
                    }
                } else {
                    if (Array.isArray(document?.writeErrors) && document.writeErrors.some((error)=>error?.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired) || document?.writeConcernError?.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired) {
                        throw new error_1.MongoOperationTimeoutError('Server reported a timeout error', {
                            cause: new error_1.MongoServerError(document)
                        });
                    }
                }
            }
            return document;
        }
        throw new error_1.MongoUnexpectedServerResponseError('Unable to get response from server');
    }
    exhaustCommand(ns, command, options, replyListener) {
        const exhaustLoop = async ()=>{
            this.throwIfAborted();
            for await (const reply of this.sendCommand(ns, command, options)){
                replyListener(undefined, reply);
                this.throwIfAborted();
            }
            throw new error_1.MongoUnexpectedServerResponseError('Server ended moreToCome unexpectedly');
        };
        exhaustLoop().then(undefined, replyListener);
    }
    throwIfAborted() {
        if (this.error) throw this.error;
    }
    /**
     * @internal
     *
     * Writes an OP_MSG or OP_QUERY request to the socket, optionally compressing the command. This method
     * waits until the socket's buffer has emptied (the Nodejs socket `drain` event has fired).
     */ async writeCommand(command, options) {
        const finalCommand = options.agreedCompressor === 'none' || !commands_1.OpCompressedRequest.canCompress(command) ? command : new commands_1.OpCompressedRequest(command, {
            agreedCompressor: options.agreedCompressor ?? 'none',
            zlibCompressionLevel: options.zlibCompressionLevel ?? 0
        });
        const buffer = Buffer.concat(await finalCommand.toBin());
        if (options.timeoutContext?.csotEnabled()) {
            if (options.timeoutContext.minRoundTripTime != null && options.timeoutContext.remainingTimeMS < options.timeoutContext.minRoundTripTime) {
                throw new error_1.MongoOperationTimeoutError('Server roundtrip time is greater than the time remaining');
            }
        }
        if (this.socket.write(buffer)) return;
        const drainEvent = (0, utils_1.once)(this.socket, 'drain', options);
        const timeout = options?.timeoutContext?.timeoutForSocketWrite;
        const drained = timeout ? Promise.race([
            drainEvent,
            timeout
        ]) : drainEvent;
        try {
            return await drained;
        } catch (writeError) {
            if (timeout_1.TimeoutError.is(writeError)) {
                const timeoutError = new error_1.MongoOperationTimeoutError('Timed out at socket write');
                this.onError(timeoutError);
                throw timeoutError;
            } else if (writeError === options.signal?.reason) {
                this.onError(writeError);
            }
            throw writeError;
        } finally{
            timeout?.clear();
        }
    }
    /**
     * @internal
     *
     * Returns an async generator that yields full wire protocol messages from the underlying socket.  This function
     * yields messages until `moreToCome` is false or not present in a response, or the caller cancels the request
     * by calling `return` on the generator.
     *
     * Note that `for-await` loops call `return` automatically when the loop is exited.
     */ async *readMany(options) {
        try {
            this.dataEvents = (0, on_data_1.onData)(this.messageStream, options);
            this.messageStream.resume();
            for await (const message of this.dataEvents){
                const response = await (0, compression_1.decompressResponse)(message);
                yield response;
                if (!response.moreToCome) {
                    return;
                }
            }
        } catch (readError) {
            if (timeout_1.TimeoutError.is(readError)) {
                const timeoutError = new error_1.MongoOperationTimeoutError(`Timed out during socket read (${readError.duration}ms)`);
                this.dataEvents = null;
                this.onError(timeoutError);
                throw timeoutError;
            } else if (readError === options.signal?.reason) {
                this.onError(readError);
            }
            throw readError;
        } finally{
            this.dataEvents = null;
            this.messageStream.pause();
        }
    }
}
exports.Connection = Connection;
/** @event */ Connection.COMMAND_STARTED = constants_1.COMMAND_STARTED;
/** @event */ Connection.COMMAND_SUCCEEDED = constants_1.COMMAND_SUCCEEDED;
/** @event */ Connection.COMMAND_FAILED = constants_1.COMMAND_FAILED;
/** @event */ Connection.CLUSTER_TIME_RECEIVED = constants_1.CLUSTER_TIME_RECEIVED;
/** @event */ Connection.CLOSE = constants_1.CLOSE;
/** @event */ Connection.PINNED = constants_1.PINNED;
/** @event */ Connection.UNPINNED = constants_1.UNPINNED;
/** @internal */ class SizedMessageTransform extends stream_1.Transform {
    constructor({ connection }){
        super({
            writableObjectMode: false,
            readableObjectMode: true
        });
        this.bufferPool = new utils_1.BufferPool();
        this.connection = connection;
    }
    _transform(chunk, encoding, callback) {
        if (this.connection.delayedTimeoutId != null) {
            (0, timers_1.clearTimeout)(this.connection.delayedTimeoutId);
            this.connection.delayedTimeoutId = null;
        }
        this.bufferPool.append(chunk);
        while(this.bufferPool.length){
            // While there are any bytes in the buffer
            // Try to fetch a size from the top 4 bytes
            const sizeOfMessage = this.bufferPool.getInt32();
            if (sizeOfMessage == null) {
                break;
            }
            if (sizeOfMessage < 0) {
                // The size in the message has a negative value, this is probably corruption, throw:
                return callback(new error_1.MongoParseError(`Message size cannot be negative: ${sizeOfMessage}`));
            }
            if (sizeOfMessage > this.bufferPool.length) {
                break;
            }
            // Add a message to the stream
            const message = this.bufferPool.read(sizeOfMessage);
            if (!this.push(message)) {
                // We only subscribe to data events so we should never get backpressure
                // if we do, we do not have the handling for it.
                return callback(new error_1.MongoRuntimeError(`SizedMessageTransform does not support backpressure`));
            }
        }
        callback();
    }
}
exports.SizedMessageTransform = SizedMessageTransform;
/** @internal */ class CryptoConnection extends Connection {
    constructor(stream, options){
        super(stream, options);
        this.autoEncrypter = options.autoEncrypter;
    }
    async command(ns, cmd, options, responseType) {
        const { autoEncrypter } = this;
        if (!autoEncrypter) {
            // TODO(NODE-6065): throw a MongoRuntimeError in Node V7
            // @ts-expect-error No cause provided because there is no underlying error.
            throw new error_1.MongoMissingDependencyError('No AutoEncrypter available for encryption', {
                dependencyName: 'n/a'
            });
        }
        const serverWireVersion = (0, utils_1.maxWireVersion)(this);
        if (serverWireVersion === 0) {
            // This means the initial handshake hasn't happened yet
            return await super.command(ns, cmd, options, responseType);
        }
        if (serverWireVersion < 8) {
            throw new error_1.MongoCompatibilityError('Auto-encryption requires a minimum MongoDB version of 4.2');
        }
        // Save sort or indexKeys based on the command being run
        // the encrypt API serializes our JS objects to BSON to pass to the native code layer
        // and then deserializes the encrypted result, the protocol level components
        // of the command (ex. sort) are then converted to JS objects potentially losing
        // import key order information. These fields are never encrypted so we can save the values
        // from before the encryption and replace them after encryption has been performed
        const sort = cmd.find || cmd.findAndModify ? cmd.sort : null;
        const indexKeys = cmd.createIndexes ? cmd.indexes.map((index)=>index.key) : null;
        const encrypted = await autoEncrypter.encrypt(ns.toString(), cmd, options);
        // Replace the saved values
        if (sort != null && (cmd.find || cmd.findAndModify)) {
            encrypted.sort = sort;
        }
        if (indexKeys != null && cmd.createIndexes) {
            for (const [offset, index] of indexKeys.entries()){
                // @ts-expect-error `encrypted` is a generic "command", but we've narrowed for only `createIndexes` commands here
                encrypted.indexes[offset].key = index;
            }
        }
        const encryptedResponse = await super.command(ns, encrypted, options, // Eventually we want to require `responseType` which means we would satisfy `T` as the return type.
        // In the meantime, we want encryptedResponse to always be _at least_ a MongoDBResponse if not a more specific subclass
        // So that we can ensure we have access to the on-demand APIs for decorate response
        responseType ?? responses_1.MongoDBResponse);
        const result = await autoEncrypter.decrypt(encryptedResponse.toBytes(), options);
        const decryptedResponse = responseType?.make(result) ?? (0, bson_1.deserialize)(result, options);
        if (autoEncrypter[constants_1.kDecorateResult]) {
            if (responseType == null) {
                (0, utils_1.decorateDecryptionResult)(decryptedResponse, encryptedResponse.toObject(), true);
            } else if (decryptedResponse instanceof responses_1.CursorResponse) {
                decryptedResponse.encryptedResponse = encryptedResponse;
            }
        }
        return decryptedResponse;
    }
}
exports.CryptoConnection = CryptoConnection; //# sourceMappingURL=connection.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/connect.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.LEGAL_TCP_SOCKET_OPTIONS = exports.LEGAL_TLS_SOCKET_OPTIONS = void 0;
exports.connect = connect;
exports.makeConnection = makeConnection;
exports.performInitialHandshake = performInitialHandshake;
exports.prepareHandshakeDocument = prepareHandshakeDocument;
exports.makeSocket = makeSocket;
const net = require("net");
const tls = require("tls");
const constants_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/constants.js [app-route] (ecmascript)");
const deps_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/deps.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const auth_provider_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [app-route] (ecmascript)");
const providers_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [app-route] (ecmascript)");
const connection_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/connection.js [app-route] (ecmascript)");
const constants_2 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [app-route] (ecmascript)");
async function connect(options) {
    let connection = null;
    try {
        const socket = await makeSocket(options);
        connection = makeConnection(options, socket);
        await performInitialHandshake(connection, options);
        return connection;
    } catch (error) {
        connection?.destroy();
        throw error;
    }
}
function makeConnection(options, socket) {
    let ConnectionType = options.connectionType ?? connection_1.Connection;
    if (options.autoEncrypter) {
        ConnectionType = connection_1.CryptoConnection;
    }
    return new ConnectionType(socket, options);
}
function checkSupportedServer(hello, options) {
    const maxWireVersion = Number(hello.maxWireVersion);
    const minWireVersion = Number(hello.minWireVersion);
    const serverVersionHighEnough = !Number.isNaN(maxWireVersion) && maxWireVersion >= constants_2.MIN_SUPPORTED_WIRE_VERSION;
    const serverVersionLowEnough = !Number.isNaN(minWireVersion) && minWireVersion <= constants_2.MAX_SUPPORTED_WIRE_VERSION;
    if (serverVersionHighEnough) {
        if (serverVersionLowEnough) {
            return null;
        }
        const message = `Server at ${options.hostAddress} reports minimum wire version ${JSON.stringify(hello.minWireVersion)}, but this version of the Node.js Driver requires at most ${constants_2.MAX_SUPPORTED_WIRE_VERSION} (MongoDB ${constants_2.MAX_SUPPORTED_SERVER_VERSION})`;
        return new error_1.MongoCompatibilityError(message);
    }
    const message = `Server at ${options.hostAddress} reports maximum wire version ${JSON.stringify(hello.maxWireVersion) ?? 0}, but this version of the Node.js Driver requires at least ${constants_2.MIN_SUPPORTED_WIRE_VERSION} (MongoDB ${constants_2.MIN_SUPPORTED_SERVER_VERSION})`;
    return new error_1.MongoCompatibilityError(message);
}
async function performInitialHandshake(conn, options) {
    const credentials = options.credentials;
    if (credentials) {
        if (!(credentials.mechanism === providers_1.AuthMechanism.MONGODB_DEFAULT) && !options.authProviders.getOrCreateProvider(credentials.mechanism, credentials.mechanismProperties)) {
            throw new error_1.MongoInvalidArgumentError(`AuthMechanism '${credentials.mechanism}' not supported`);
        }
    }
    const authContext = new auth_provider_1.AuthContext(conn, credentials, options);
    conn.authContext = authContext;
    const handshakeDoc = await prepareHandshakeDocument(authContext);
    // @ts-expect-error: TODO(NODE-5141): The options need to be filtered properly, Connection options differ from Command options
    const handshakeOptions = {
        ...options,
        raw: false
    };
    if (typeof options.connectTimeoutMS === 'number') {
        // The handshake technically is a monitoring check, so its socket timeout should be connectTimeoutMS
        handshakeOptions.socketTimeoutMS = options.connectTimeoutMS;
    }
    const start = new Date().getTime();
    const response = await executeHandshake(handshakeDoc, handshakeOptions);
    if (!('isWritablePrimary' in response)) {
        // Provide hello-style response document.
        response.isWritablePrimary = response[constants_1.LEGACY_HELLO_COMMAND];
    }
    if (response.helloOk) {
        conn.helloOk = true;
    }
    const supportedServerErr = checkSupportedServer(response, options);
    if (supportedServerErr) {
        throw supportedServerErr;
    }
    if (options.loadBalanced) {
        if (!response.serviceId) {
            throw new error_1.MongoCompatibilityError('Driver attempted to initialize in load balancing mode, ' + 'but the server does not support this mode.');
        }
    }
    // NOTE: This is metadata attached to the connection while porting away from
    //       handshake being done in the `Server` class. Likely, it should be
    //       relocated, or at very least restructured.
    conn.hello = response;
    conn.lastHelloMS = new Date().getTime() - start;
    if (!response.arbiterOnly && credentials) {
        // store the response on auth context
        authContext.response = response;
        const resolvedCredentials = credentials.resolveAuthMechanism(response);
        const provider = options.authProviders.getOrCreateProvider(resolvedCredentials.mechanism, resolvedCredentials.mechanismProperties);
        if (!provider) {
            throw new error_1.MongoInvalidArgumentError(`No AuthProvider for ${resolvedCredentials.mechanism} defined.`);
        }
        try {
            await provider.auth(authContext);
        } catch (error) {
            if (error instanceof error_1.MongoError) {
                error.addErrorLabel(error_1.MongoErrorLabel.HandshakeError);
                if ((0, error_1.needsRetryableWriteLabel)(error, response.maxWireVersion, conn.description.type)) {
                    error.addErrorLabel(error_1.MongoErrorLabel.RetryableWriteError);
                }
            }
            throw error;
        }
    }
    // Connection establishment is socket creation (tcp handshake, tls handshake, MongoDB handshake (saslStart, saslContinue))
    // Once connection is established, command logging can log events (if enabled)
    conn.established = true;
    async function executeHandshake(handshakeDoc, handshakeOptions) {
        try {
            const handshakeResponse = await conn.command((0, utils_1.ns)('admin.$cmd'), handshakeDoc, handshakeOptions);
            return handshakeResponse;
        } catch (error) {
            if (error instanceof error_1.MongoError) {
                error.addErrorLabel(error_1.MongoErrorLabel.HandshakeError);
            }
            throw error;
        }
    }
}
/**
 * @internal
 *
 * This function is only exposed for testing purposes.
 */ async function prepareHandshakeDocument(authContext) {
    const options = authContext.options;
    const compressors = options.compressors ? options.compressors : [];
    const { serverApi } = authContext.connection;
    const clientMetadata = await options.extendedMetadata;
    const handshakeDoc = {
        [serverApi?.version || options.loadBalanced === true ? 'hello' : constants_1.LEGACY_HELLO_COMMAND]: 1,
        helloOk: true,
        client: clientMetadata,
        compression: compressors
    };
    if (options.loadBalanced === true) {
        handshakeDoc.loadBalanced = true;
    }
    const credentials = authContext.credentials;
    if (credentials) {
        if (credentials.mechanism === providers_1.AuthMechanism.MONGODB_DEFAULT && credentials.username) {
            handshakeDoc.saslSupportedMechs = `${credentials.source}.${credentials.username}`;
            const provider = authContext.options.authProviders.getOrCreateProvider(providers_1.AuthMechanism.MONGODB_SCRAM_SHA256, credentials.mechanismProperties);
            if (!provider) {
                // This auth mechanism is always present.
                throw new error_1.MongoInvalidArgumentError(`No AuthProvider for ${providers_1.AuthMechanism.MONGODB_SCRAM_SHA256} defined.`);
            }
            return await provider.prepare(handshakeDoc, authContext);
        }
        const provider = authContext.options.authProviders.getOrCreateProvider(credentials.mechanism, credentials.mechanismProperties);
        if (!provider) {
            throw new error_1.MongoInvalidArgumentError(`No AuthProvider for ${credentials.mechanism} defined.`);
        }
        return await provider.prepare(handshakeDoc, authContext);
    }
    return handshakeDoc;
}
/** @public */ exports.LEGAL_TLS_SOCKET_OPTIONS = [
    'allowPartialTrustChain',
    'ALPNProtocols',
    'ca',
    'cert',
    'checkServerIdentity',
    'ciphers',
    'crl',
    'ecdhCurve',
    'key',
    'minDHSize',
    'passphrase',
    'pfx',
    'rejectUnauthorized',
    'secureContext',
    'secureProtocol',
    'servername',
    'session'
];
/** @public */ exports.LEGAL_TCP_SOCKET_OPTIONS = [
    'autoSelectFamily',
    'autoSelectFamilyAttemptTimeout',
    'keepAliveInitialDelay',
    'family',
    'hints',
    'localAddress',
    'localPort',
    'lookup'
];
function parseConnectOptions(options) {
    const hostAddress = options.hostAddress;
    if (!hostAddress) throw new error_1.MongoInvalidArgumentError('Option "hostAddress" is required');
    const result = {};
    for (const name of exports.LEGAL_TCP_SOCKET_OPTIONS){
        if (options[name] != null) {
            result[name] = options[name];
        }
    }
    result.keepAliveInitialDelay ??= 120000;
    result.keepAlive = true;
    result.noDelay = options.noDelay ?? true;
    if (typeof hostAddress.socketPath === 'string') {
        result.path = hostAddress.socketPath;
        return result;
    } else if (typeof hostAddress.host === 'string') {
        result.host = hostAddress.host;
        result.port = hostAddress.port;
        return result;
    } else {
        // This should never happen since we set up HostAddresses
        // But if we don't throw here the socket could hang until timeout
        // TODO(NODE-3483)
        throw new error_1.MongoRuntimeError(`Unexpected HostAddress ${JSON.stringify(hostAddress)}`);
    }
}
function parseSslOptions(options) {
    const result = parseConnectOptions(options);
    // Merge in valid SSL options
    for (const name of exports.LEGAL_TLS_SOCKET_OPTIONS){
        if (options[name] != null) {
            result[name] = options[name];
        }
    }
    if (options.existingSocket) {
        result.socket = options.existingSocket;
    }
    // Set default sni servername to be the same as host
    if (result.servername == null && result.host && !net.isIP(result.host)) {
        result.servername = result.host;
    }
    return result;
}
async function makeSocket(options) {
    const useTLS = options.tls ?? false;
    const connectTimeoutMS = options.connectTimeoutMS ?? 30000;
    const existingSocket = options.existingSocket;
    let socket;
    if (options.proxyHost != null) {
        // Currently, only Socks5 is supported.
        return await makeSocks5Connection({
            ...options,
            connectTimeoutMS
        });
    }
    if (useTLS) {
        const tlsSocket = tls.connect(parseSslOptions(options));
        if (typeof tlsSocket.disableRenegotiation === 'function') {
            tlsSocket.disableRenegotiation();
        }
        socket = tlsSocket;
    } else if (existingSocket) {
        // In the TLS case, parseSslOptions() sets options.socket to existingSocket,
        // so we only need to handle the non-TLS case here (where existingSocket
        // gives us all we need out of the box).
        socket = existingSocket;
    } else {
        socket = net.createConnection(parseConnectOptions(options));
    }
    socket.setTimeout(connectTimeoutMS);
    let cancellationHandler = null;
    const { promise: connectedSocket, resolve, reject } = (0, utils_1.promiseWithResolvers)();
    if (existingSocket) {
        resolve(socket);
    } else {
        const start = performance.now();
        const connectEvent = useTLS ? 'secureConnect' : 'connect';
        socket.once(connectEvent, ()=>resolve(socket)).once('error', (cause)=>reject(new error_1.MongoNetworkError(error_1.MongoError.buildErrorMessage(cause), {
                cause
            }))).once('timeout', ()=>{
            reject(new error_1.MongoNetworkTimeoutError(`Socket '${connectEvent}' timed out after ${performance.now() - start | 0}ms (connectTimeoutMS: ${connectTimeoutMS})`));
        }).once('close', ()=>reject(new error_1.MongoNetworkError(`Socket closed after ${performance.now() - start | 0} during connection establishment`)));
        if (options.cancellationToken != null) {
            cancellationHandler = ()=>reject(new error_1.MongoNetworkError(`Socket connection establishment was cancelled after ${performance.now() - start | 0}`));
            options.cancellationToken.once('cancel', cancellationHandler);
        }
    }
    try {
        socket = await connectedSocket;
        return socket;
    } catch (error) {
        socket.destroy();
        throw error;
    } finally{
        socket.setTimeout(0);
        if (cancellationHandler != null) {
            options.cancellationToken?.removeListener('cancel', cancellationHandler);
        }
    }
}
let socks = null;
function loadSocks() {
    if (socks == null) {
        const socksImport = (0, deps_1.getSocks)();
        if ('kModuleError' in socksImport) {
            throw socksImport.kModuleError;
        }
        socks = socksImport;
    }
    return socks;
}
async function makeSocks5Connection(options) {
    const hostAddress = utils_1.HostAddress.fromHostPort(options.proxyHost ?? '', options.proxyPort ?? 1080);
    // First, connect to the proxy server itself:
    const rawSocket = await makeSocket({
        ...options,
        hostAddress,
        tls: false,
        proxyHost: undefined
    });
    const destination = parseConnectOptions(options);
    if (typeof destination.host !== 'string' || typeof destination.port !== 'number') {
        throw new error_1.MongoInvalidArgumentError('Can only make Socks5 connections to TCP hosts');
    }
    socks ??= loadSocks();
    let existingSocket;
    try {
        // Then, establish the Socks5 proxy connection:
        const connection = await socks.SocksClient.createConnection({
            existing_socket: rawSocket,
            timeout: options.connectTimeoutMS,
            command: 'connect',
            destination: {
                host: destination.host,
                port: destination.port
            },
            proxy: {
                // host and port are ignored because we pass existing_socket
                host: 'iLoveJavaScript',
                port: 0,
                type: 5,
                userId: options.proxyUsername || undefined,
                password: options.proxyPassword || undefined
            }
        });
        existingSocket = connection.socket;
    } catch (cause) {
        throw new error_1.MongoNetworkError(error_1.MongoError.buildErrorMessage(cause), {
            cause
        });
    }
    // Finally, now treat the resulting duplex stream as the
    // socket over which we send and receive wire protocol messages:
    return await makeSocket({
        ...options,
        existingSocket,
        proxyHost: undefined
    });
} //# sourceMappingURL=connect.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/connection_pool_events.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ConnectionPoolClearedEvent = exports.ConnectionCheckedInEvent = exports.ConnectionCheckedOutEvent = exports.ConnectionCheckOutFailedEvent = exports.ConnectionCheckOutStartedEvent = exports.ConnectionClosedEvent = exports.ConnectionReadyEvent = exports.ConnectionCreatedEvent = exports.ConnectionPoolClosedEvent = exports.ConnectionPoolReadyEvent = exports.ConnectionPoolCreatedEvent = exports.ConnectionPoolMonitoringEvent = void 0;
const constants_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/constants.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
/**
 * The base export class for all monitoring events published from the connection pool
 * @public
 * @category Event
 */ class ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        this.time = new Date();
        this.address = pool.address;
    }
}
exports.ConnectionPoolMonitoringEvent = ConnectionPoolMonitoringEvent;
/**
 * An event published when a connection pool is created
 * @public
 * @category Event
 */ class ConnectionPoolCreatedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_POOL_CREATED;
        const { maxConnecting, maxPoolSize, minPoolSize, maxIdleTimeMS, waitQueueTimeoutMS } = pool.options;
        this.options = {
            maxConnecting,
            maxPoolSize,
            minPoolSize,
            maxIdleTimeMS,
            waitQueueTimeoutMS
        };
    }
}
exports.ConnectionPoolCreatedEvent = ConnectionPoolCreatedEvent;
/**
 * An event published when a connection pool is ready
 * @public
 * @category Event
 */ class ConnectionPoolReadyEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_POOL_READY;
    }
}
exports.ConnectionPoolReadyEvent = ConnectionPoolReadyEvent;
/**
 * An event published when a connection pool is closed
 * @public
 * @category Event
 */ class ConnectionPoolClosedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_POOL_CLOSED;
    }
}
exports.ConnectionPoolClosedEvent = ConnectionPoolClosedEvent;
/**
 * An event published when a connection pool creates a new connection
 * @public
 * @category Event
 */ class ConnectionCreatedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CREATED;
        this.connectionId = connection.id;
    }
}
exports.ConnectionCreatedEvent = ConnectionCreatedEvent;
/**
 * An event published when a connection is ready for use
 * @public
 * @category Event
 */ class ConnectionReadyEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection, connectionCreatedEventTime){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_READY;
        this.durationMS = (0, utils_1.now)() - connectionCreatedEventTime;
        this.connectionId = connection.id;
    }
}
exports.ConnectionReadyEvent = ConnectionReadyEvent;
/**
 * An event published when a connection is closed
 * @public
 * @category Event
 */ class ConnectionClosedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection, reason, error){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CLOSED;
        this.connectionId = connection.id;
        this.reason = reason;
        this.serviceId = connection.serviceId;
        this.error = error ?? null;
    }
}
exports.ConnectionClosedEvent = ConnectionClosedEvent;
/**
 * An event published when a request to check a connection out begins
 * @public
 * @category Event
 */ class ConnectionCheckOutStartedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CHECK_OUT_STARTED;
    }
}
exports.ConnectionCheckOutStartedEvent = ConnectionCheckOutStartedEvent;
/**
 * An event published when a request to check a connection out fails
 * @public
 * @category Event
 */ class ConnectionCheckOutFailedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, reason, checkoutTime, error){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CHECK_OUT_FAILED;
        this.durationMS = (0, utils_1.now)() - checkoutTime;
        this.reason = reason;
        this.error = error;
    }
}
exports.ConnectionCheckOutFailedEvent = ConnectionCheckOutFailedEvent;
/**
 * An event published when a connection is checked out of the connection pool
 * @public
 * @category Event
 */ class ConnectionCheckedOutEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection, checkoutTime){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CHECKED_OUT;
        this.durationMS = (0, utils_1.now)() - checkoutTime;
        this.connectionId = connection.id;
    }
}
exports.ConnectionCheckedOutEvent = ConnectionCheckedOutEvent;
/**
 * An event published when a connection is checked into the connection pool
 * @public
 * @category Event
 */ class ConnectionCheckedInEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CHECKED_IN;
        this.connectionId = connection.id;
    }
}
exports.ConnectionCheckedInEvent = ConnectionCheckedInEvent;
/**
 * An event published when a connection pool is cleared
 * @public
 * @category Event
 */ class ConnectionPoolClearedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, options = {}){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_POOL_CLEARED;
        this.serviceId = options.serviceId;
        this.interruptInUseConnections = options.interruptInUseConnections;
    }
}
exports.ConnectionPoolClearedEvent = ConnectionPoolClearedEvent; //# sourceMappingURL=connection_pool_events.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/errors.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.WaitQueueTimeoutError = exports.PoolClearedOnNetworkError = exports.PoolClearedError = exports.PoolClosedError = void 0;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
/**
 * An error indicating a connection pool is closed
 * @category Error
 */ class PoolClosedError extends error_1.MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(pool){
        super('Attempted to check out a connection from closed connection pool');
        this.address = pool.address;
    }
    get name() {
        return 'MongoPoolClosedError';
    }
}
exports.PoolClosedError = PoolClosedError;
/**
 * An error indicating a connection pool is currently paused
 * @category Error
 */ class PoolClearedError extends error_1.MongoNetworkError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(pool, message){
        const errorMessage = message ? message : `Connection pool for ${pool.address} was cleared because another operation failed with: "${pool.serverError?.message}"`;
        super(errorMessage, pool.serverError ? {
            cause: pool.serverError
        } : undefined);
        this.address = pool.address;
        this.addErrorLabel(error_1.MongoErrorLabel.PoolRequstedRetry);
    }
    get name() {
        return 'MongoPoolClearedError';
    }
}
exports.PoolClearedError = PoolClearedError;
/**
 * An error indicating that a connection pool has been cleared after the monitor for that server timed out.
 * @category Error
 */ class PoolClearedOnNetworkError extends PoolClearedError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(pool){
        super(pool, `Connection to ${pool.address} interrupted due to server monitor timeout`);
    }
    get name() {
        return 'PoolClearedOnNetworkError';
    }
}
exports.PoolClearedOnNetworkError = PoolClearedOnNetworkError;
/**
 * An error thrown when a request to check out a connection times out
 * @category Error
 */ class WaitQueueTimeoutError extends error_1.MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, address){
        super(message);
        this.address = address;
    }
    get name() {
        return 'MongoWaitQueueTimeoutError';
    }
}
exports.WaitQueueTimeoutError = WaitQueueTimeoutError; //# sourceMappingURL=errors.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/connection_pool.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ConnectionPool = exports.PoolState = void 0;
const timers_1 = require("timers");
const constants_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/constants.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const mongo_types_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/mongo_types.js [app-route] (ecmascript)");
const timeout_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/timeout.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const connect_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/connect.js [app-route] (ecmascript)");
const connection_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/connection.js [app-route] (ecmascript)");
const connection_pool_events_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/connection_pool_events.js [app-route] (ecmascript)");
const errors_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/errors.js [app-route] (ecmascript)");
const metrics_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/metrics.js [app-route] (ecmascript)");
/** @internal */ exports.PoolState = Object.freeze({
    paused: 'paused',
    ready: 'ready',
    closed: 'closed'
});
/**
 * A pool of connections which dynamically resizes, and emit events related to pool activity
 * @internal
 */ class ConnectionPool extends mongo_types_1.TypedEventEmitter {
    constructor(server, options){
        super();
        this.on('error', utils_1.noop);
        this.options = Object.freeze({
            connectionType: connection_1.Connection,
            ...options,
            maxPoolSize: options.maxPoolSize ?? 100,
            minPoolSize: options.minPoolSize ?? 0,
            maxConnecting: options.maxConnecting ?? 2,
            maxIdleTimeMS: options.maxIdleTimeMS ?? 0,
            waitQueueTimeoutMS: options.waitQueueTimeoutMS ?? 0,
            minPoolSizeCheckFrequencyMS: options.minPoolSizeCheckFrequencyMS ?? 100,
            autoEncrypter: options.autoEncrypter
        });
        if (this.options.minPoolSize > this.options.maxPoolSize) {
            throw new error_1.MongoInvalidArgumentError('Connection pool minimum size must not be greater than maximum pool size');
        }
        this.poolState = exports.PoolState.paused;
        this.server = server;
        this.connections = new utils_1.List();
        this.pending = 0;
        this.checkedOut = new Set();
        this.minPoolSizeTimer = undefined;
        this.generation = 0;
        this.serviceGenerations = new Map();
        this.connectionCounter = (0, utils_1.makeCounter)(1);
        this.cancellationToken = new mongo_types_1.CancellationToken();
        this.cancellationToken.setMaxListeners(Infinity);
        this.waitQueue = new utils_1.List();
        this.metrics = new metrics_1.ConnectionPoolMetrics();
        this.processingWaitQueue = false;
        this.mongoLogger = this.server.topology.client?.mongoLogger;
        this.component = 'connection';
        process.nextTick(()=>{
            this.emitAndLog(ConnectionPool.CONNECTION_POOL_CREATED, new connection_pool_events_1.ConnectionPoolCreatedEvent(this));
        });
    }
    /** The address of the endpoint the pool is connected to */ get address() {
        return this.options.hostAddress.toString();
    }
    /**
     * Check if the pool has been closed
     *
     * TODO(NODE-3263): We can remove this property once shell no longer needs it
     */ get closed() {
        return this.poolState === exports.PoolState.closed;
    }
    /** An integer expressing how many total connections (available + pending + in use) the pool currently has */ get totalConnectionCount() {
        return this.availableConnectionCount + this.pendingConnectionCount + this.currentCheckedOutCount;
    }
    /** An integer expressing how many connections are currently available in the pool. */ get availableConnectionCount() {
        return this.connections.length;
    }
    get pendingConnectionCount() {
        return this.pending;
    }
    get currentCheckedOutCount() {
        return this.checkedOut.size;
    }
    get waitQueueSize() {
        return this.waitQueue.length;
    }
    get loadBalanced() {
        return this.options.loadBalanced;
    }
    get serverError() {
        return this.server.description.error;
    }
    /**
     * This is exposed ONLY for use in mongosh, to enable
     * killing all connections if a user quits the shell with
     * operations in progress.
     *
     * This property may be removed as a part of NODE-3263.
     */ get checkedOutConnections() {
        return this.checkedOut;
    }
    /**
     * Get the metrics information for the pool when a wait queue timeout occurs.
     */ waitQueueErrorMetrics() {
        return this.metrics.info(this.options.maxPoolSize);
    }
    /**
     * Set the pool state to "ready"
     */ ready() {
        if (this.poolState !== exports.PoolState.paused) {
            return;
        }
        this.poolState = exports.PoolState.ready;
        this.emitAndLog(ConnectionPool.CONNECTION_POOL_READY, new connection_pool_events_1.ConnectionPoolReadyEvent(this));
        (0, timers_1.clearTimeout)(this.minPoolSizeTimer);
        this.ensureMinPoolSize();
    }
    /**
     * Check a connection out of this pool. The connection will continue to be tracked, but no reference to it
     * will be held by the pool. This means that if a connection is checked out it MUST be checked back in or
     * explicitly destroyed by the new owner.
     */ async checkOut(options) {
        const checkoutTime = (0, utils_1.now)();
        this.emitAndLog(ConnectionPool.CONNECTION_CHECK_OUT_STARTED, new connection_pool_events_1.ConnectionCheckOutStartedEvent(this));
        const { promise, resolve, reject } = (0, utils_1.promiseWithResolvers)();
        const timeout = options.timeoutContext.connectionCheckoutTimeout;
        const waitQueueMember = {
            resolve,
            reject,
            cancelled: false,
            checkoutTime
        };
        const abortListener = (0, utils_1.addAbortListener)(options.signal, function() {
            waitQueueMember.cancelled = true;
            reject(this.reason);
        });
        this.waitQueue.push(waitQueueMember);
        process.nextTick(()=>this.processWaitQueue());
        try {
            timeout?.throwIfExpired();
            return await (timeout ? Promise.race([
                promise,
                timeout
            ]) : promise);
        } catch (error) {
            if (timeout_1.TimeoutError.is(error)) {
                timeout?.clear();
                waitQueueMember.cancelled = true;
                this.emitAndLog(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, new connection_pool_events_1.ConnectionCheckOutFailedEvent(this, 'timeout', waitQueueMember.checkoutTime));
                const timeoutError = new errors_1.WaitQueueTimeoutError(this.loadBalanced ? this.waitQueueErrorMetrics() : 'Timed out while checking out a connection from connection pool', this.address);
                if (options.timeoutContext.csotEnabled()) {
                    throw new error_1.MongoOperationTimeoutError('Timed out during connection checkout', {
                        cause: timeoutError
                    });
                }
                throw timeoutError;
            }
            throw error;
        } finally{
            abortListener?.[utils_1.kDispose]();
            timeout?.clear();
        }
    }
    /**
     * Check a connection into the pool.
     *
     * @param connection - The connection to check in
     */ checkIn(connection) {
        if (!this.checkedOut.has(connection)) {
            return;
        }
        const poolClosed = this.closed;
        const stale = this.connectionIsStale(connection);
        const willDestroy = !!(poolClosed || stale || connection.closed);
        if (!willDestroy) {
            connection.markAvailable();
            this.connections.unshift(connection);
        }
        this.checkedOut.delete(connection);
        this.emitAndLog(ConnectionPool.CONNECTION_CHECKED_IN, new connection_pool_events_1.ConnectionCheckedInEvent(this, connection));
        if (willDestroy) {
            const reason = connection.closed ? 'error' : poolClosed ? 'poolClosed' : 'stale';
            this.destroyConnection(connection, reason);
        }
        process.nextTick(()=>this.processWaitQueue());
    }
    /**
     * Clear the pool
     *
     * Pool reset is handled by incrementing the pool's generation count. Any existing connection of a
     * previous generation will eventually be pruned during subsequent checkouts.
     */ clear(options = {}) {
        if (this.closed) {
            return;
        }
        // handle load balanced case
        if (this.loadBalanced) {
            const { serviceId } = options;
            if (!serviceId) {
                throw new error_1.MongoRuntimeError('ConnectionPool.clear() called in load balanced mode with no serviceId.');
            }
            const sid = serviceId.toHexString();
            const generation = this.serviceGenerations.get(sid);
            // Only need to worry if the generation exists, since it should
            // always be there but typescript needs the check.
            if (generation == null) {
                throw new error_1.MongoRuntimeError('Service generations are required in load balancer mode.');
            } else {
                // Increment the generation for the service id.
                this.serviceGenerations.set(sid, generation + 1);
            }
            this.emitAndLog(ConnectionPool.CONNECTION_POOL_CLEARED, new connection_pool_events_1.ConnectionPoolClearedEvent(this, {
                serviceId
            }));
            return;
        }
        // handle non load-balanced case
        const interruptInUseConnections = options.interruptInUseConnections ?? false;
        const oldGeneration = this.generation;
        this.generation += 1;
        const alreadyPaused = this.poolState === exports.PoolState.paused;
        this.poolState = exports.PoolState.paused;
        this.clearMinPoolSizeTimer();
        if (!alreadyPaused) {
            this.emitAndLog(ConnectionPool.CONNECTION_POOL_CLEARED, new connection_pool_events_1.ConnectionPoolClearedEvent(this, {
                interruptInUseConnections
            }));
        }
        if (interruptInUseConnections) {
            process.nextTick(()=>this.interruptInUseConnections(oldGeneration));
        }
        this.processWaitQueue();
    }
    /**
     * Closes all stale in-use connections in the pool with a resumable PoolClearedOnNetworkError.
     *
     * Only connections where `connection.generation <= minGeneration` are killed.
     */ interruptInUseConnections(minGeneration) {
        for (const connection of this.checkedOut){
            if (connection.generation <= minGeneration) {
                connection.onError(new errors_1.PoolClearedOnNetworkError(this));
            }
        }
    }
    /** For MongoClient.close() procedures */ closeCheckedOutConnections() {
        for (const conn of this.checkedOut){
            conn.onError(new error_1.MongoClientClosedError());
        }
    }
    /** Close the pool */ close() {
        if (this.closed) {
            return;
        }
        // immediately cancel any in-flight connections
        this.cancellationToken.emit('cancel');
        // end the connection counter
        if (typeof this.connectionCounter.return === 'function') {
            this.connectionCounter.return(undefined);
        }
        this.poolState = exports.PoolState.closed;
        this.clearMinPoolSizeTimer();
        this.processWaitQueue();
        for (const conn of this.connections){
            this.emitAndLog(ConnectionPool.CONNECTION_CLOSED, new connection_pool_events_1.ConnectionClosedEvent(this, conn, 'poolClosed'));
            conn.destroy();
        }
        this.connections.clear();
        this.emitAndLog(ConnectionPool.CONNECTION_POOL_CLOSED, new connection_pool_events_1.ConnectionPoolClosedEvent(this));
    }
    /**
     * @internal
     * Reauthenticate a connection
     */ async reauthenticate(connection) {
        const authContext = connection.authContext;
        if (!authContext) {
            throw new error_1.MongoRuntimeError('No auth context found on connection.');
        }
        const credentials = authContext.credentials;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('Connection is missing credentials when asked to reauthenticate');
        }
        const resolvedCredentials = credentials.resolveAuthMechanism(connection.hello);
        const provider = this.server.topology.client.s.authProviders.getOrCreateProvider(resolvedCredentials.mechanism, resolvedCredentials.mechanismProperties);
        if (!provider) {
            throw new error_1.MongoMissingCredentialsError(`Reauthenticate failed due to no auth provider for ${credentials.mechanism}`);
        }
        await provider.reauth(authContext);
        return;
    }
    /** Clear the min pool size timer */ clearMinPoolSizeTimer() {
        const minPoolSizeTimer = this.minPoolSizeTimer;
        if (minPoolSizeTimer) {
            (0, timers_1.clearTimeout)(minPoolSizeTimer);
        }
    }
    destroyConnection(connection, reason) {
        this.emitAndLog(ConnectionPool.CONNECTION_CLOSED, new connection_pool_events_1.ConnectionClosedEvent(this, connection, reason));
        // destroy the connection
        connection.destroy();
    }
    connectionIsStale(connection) {
        const serviceId = connection.serviceId;
        if (this.loadBalanced && serviceId) {
            const sid = serviceId.toHexString();
            const generation = this.serviceGenerations.get(sid);
            return connection.generation !== generation;
        }
        return connection.generation !== this.generation;
    }
    connectionIsIdle(connection) {
        return !!(this.options.maxIdleTimeMS && connection.idleTime > this.options.maxIdleTimeMS);
    }
    /**
     * Destroys a connection if the connection is perished.
     *
     * @returns `true` if the connection was destroyed, `false` otherwise.
     */ destroyConnectionIfPerished(connection) {
        const isStale = this.connectionIsStale(connection);
        const isIdle = this.connectionIsIdle(connection);
        if (!isStale && !isIdle && !connection.closed) {
            return false;
        }
        const reason = connection.closed ? 'error' : isStale ? 'stale' : 'idle';
        this.destroyConnection(connection, reason);
        return true;
    }
    createConnection(callback) {
        const connectOptions = {
            ...this.options,
            id: this.connectionCounter.next().value,
            generation: this.generation,
            cancellationToken: this.cancellationToken,
            mongoLogger: this.mongoLogger,
            authProviders: this.server.topology.client.s.authProviders
        };
        this.pending++;
        // This is our version of a "virtual" no-I/O connection as the spec requires
        const connectionCreatedTime = (0, utils_1.now)();
        this.emitAndLog(ConnectionPool.CONNECTION_CREATED, new connection_pool_events_1.ConnectionCreatedEvent(this, {
            id: connectOptions.id
        }));
        (0, connect_1.connect)(connectOptions).then((connection)=>{
            // The pool might have closed since we started trying to create a connection
            if (this.poolState !== exports.PoolState.ready) {
                this.pending--;
                connection.destroy();
                callback(this.closed ? new errors_1.PoolClosedError(this) : new errors_1.PoolClearedError(this));
                return;
            }
            // forward all events from the connection to the pool
            for (const event of [
                ...constants_1.APM_EVENTS,
                connection_1.Connection.CLUSTER_TIME_RECEIVED
            ]){
                connection.on(event, (e)=>this.emit(event, e));
            }
            if (this.loadBalanced) {
                connection.on(connection_1.Connection.PINNED, (pinType)=>this.metrics.markPinned(pinType));
                connection.on(connection_1.Connection.UNPINNED, (pinType)=>this.metrics.markUnpinned(pinType));
                const serviceId = connection.serviceId;
                if (serviceId) {
                    let generation;
                    const sid = serviceId.toHexString();
                    if (generation = this.serviceGenerations.get(sid)) {
                        connection.generation = generation;
                    } else {
                        this.serviceGenerations.set(sid, 0);
                        connection.generation = 0;
                    }
                }
            }
            connection.markAvailable();
            this.emitAndLog(ConnectionPool.CONNECTION_READY, new connection_pool_events_1.ConnectionReadyEvent(this, connection, connectionCreatedTime));
            this.pending--;
            callback(undefined, connection);
        }, (error)=>{
            this.pending--;
            this.server.handleError(error);
            this.emitAndLog(ConnectionPool.CONNECTION_CLOSED, new connection_pool_events_1.ConnectionClosedEvent(this, {
                id: connectOptions.id,
                serviceId: undefined
            }, 'error', // TODO(NODE-5192): Remove this cast
            error));
            if (error instanceof error_1.MongoNetworkError || error instanceof error_1.MongoServerError) {
                error.connectionGeneration = connectOptions.generation;
            }
            callback(error ?? new error_1.MongoRuntimeError('Connection creation failed without error'));
        });
    }
    ensureMinPoolSize() {
        const minPoolSize = this.options.minPoolSize;
        if (this.poolState !== exports.PoolState.ready || minPoolSize === 0) {
            return;
        }
        this.connections.prune((connection)=>this.destroyConnectionIfPerished(connection));
        if (this.totalConnectionCount < minPoolSize && this.pendingConnectionCount < this.options.maxConnecting) {
            // NOTE: ensureMinPoolSize should not try to get all the pending
            // connection permits because that potentially delays the availability of
            // the connection to a checkout request
            this.createConnection((err, connection)=>{
                if (!err && connection) {
                    this.connections.push(connection);
                    process.nextTick(()=>this.processWaitQueue());
                }
                if (this.poolState === exports.PoolState.ready) {
                    (0, timers_1.clearTimeout)(this.minPoolSizeTimer);
                    this.minPoolSizeTimer = (0, timers_1.setTimeout)(()=>this.ensureMinPoolSize(), this.options.minPoolSizeCheckFrequencyMS);
                }
            });
        } else {
            (0, timers_1.clearTimeout)(this.minPoolSizeTimer);
            this.minPoolSizeTimer = (0, timers_1.setTimeout)(()=>this.ensureMinPoolSize(), this.options.minPoolSizeCheckFrequencyMS);
        }
    }
    processWaitQueue() {
        if (this.processingWaitQueue) {
            return;
        }
        this.processingWaitQueue = true;
        while(this.waitQueueSize){
            const waitQueueMember = this.waitQueue.first();
            if (!waitQueueMember) {
                this.waitQueue.shift();
                continue;
            }
            if (waitQueueMember.cancelled) {
                this.waitQueue.shift();
                continue;
            }
            if (this.poolState !== exports.PoolState.ready) {
                const reason = this.closed ? 'poolClosed' : 'connectionError';
                const error = this.closed ? new errors_1.PoolClosedError(this) : new errors_1.PoolClearedError(this);
                this.emitAndLog(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, new connection_pool_events_1.ConnectionCheckOutFailedEvent(this, reason, waitQueueMember.checkoutTime, error));
                this.waitQueue.shift();
                waitQueueMember.reject(error);
                continue;
            }
            if (!this.availableConnectionCount) {
                break;
            }
            const connection = this.connections.shift();
            if (!connection) {
                break;
            }
            if (!this.destroyConnectionIfPerished(connection)) {
                this.checkedOut.add(connection);
                this.emitAndLog(ConnectionPool.CONNECTION_CHECKED_OUT, new connection_pool_events_1.ConnectionCheckedOutEvent(this, connection, waitQueueMember.checkoutTime));
                this.waitQueue.shift();
                waitQueueMember.resolve(connection);
            }
        }
        const { maxPoolSize, maxConnecting } = this.options;
        while(this.waitQueueSize > 0 && this.pendingConnectionCount < maxConnecting && (maxPoolSize === 0 || this.totalConnectionCount < maxPoolSize)){
            const waitQueueMember = this.waitQueue.shift();
            if (!waitQueueMember || waitQueueMember.cancelled) {
                continue;
            }
            this.createConnection((err, connection)=>{
                if (waitQueueMember.cancelled) {
                    if (!err && connection) {
                        this.connections.push(connection);
                    }
                } else {
                    if (err) {
                        this.emitAndLog(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, // TODO(NODE-5192): Remove this cast
                        new connection_pool_events_1.ConnectionCheckOutFailedEvent(this, 'connectionError', waitQueueMember.checkoutTime, err));
                        waitQueueMember.reject(err);
                    } else if (connection) {
                        this.checkedOut.add(connection);
                        this.emitAndLog(ConnectionPool.CONNECTION_CHECKED_OUT, new connection_pool_events_1.ConnectionCheckedOutEvent(this, connection, waitQueueMember.checkoutTime));
                        waitQueueMember.resolve(connection);
                    }
                }
                process.nextTick(()=>this.processWaitQueue());
            });
        }
        this.processingWaitQueue = false;
    }
}
exports.ConnectionPool = ConnectionPool;
/**
 * Emitted when the connection pool is created.
 * @event
 */ ConnectionPool.CONNECTION_POOL_CREATED = constants_1.CONNECTION_POOL_CREATED;
/**
 * Emitted once when the connection pool is closed
 * @event
 */ ConnectionPool.CONNECTION_POOL_CLOSED = constants_1.CONNECTION_POOL_CLOSED;
/**
 * Emitted each time the connection pool is cleared and it's generation incremented
 * @event
 */ ConnectionPool.CONNECTION_POOL_CLEARED = constants_1.CONNECTION_POOL_CLEARED;
/**
 * Emitted each time the connection pool is marked ready
 * @event
 */ ConnectionPool.CONNECTION_POOL_READY = constants_1.CONNECTION_POOL_READY;
/**
 * Emitted when a connection is created.
 * @event
 */ ConnectionPool.CONNECTION_CREATED = constants_1.CONNECTION_CREATED;
/**
 * Emitted when a connection becomes established, and is ready to use
 * @event
 */ ConnectionPool.CONNECTION_READY = constants_1.CONNECTION_READY;
/**
 * Emitted when a connection is closed
 * @event
 */ ConnectionPool.CONNECTION_CLOSED = constants_1.CONNECTION_CLOSED;
/**
 * Emitted when an attempt to check out a connection begins
 * @event
 */ ConnectionPool.CONNECTION_CHECK_OUT_STARTED = constants_1.CONNECTION_CHECK_OUT_STARTED;
/**
 * Emitted when an attempt to check out a connection fails
 * @event
 */ ConnectionPool.CONNECTION_CHECK_OUT_FAILED = constants_1.CONNECTION_CHECK_OUT_FAILED;
/**
 * Emitted each time a connection is successfully checked out of the connection pool
 * @event
 */ ConnectionPool.CONNECTION_CHECKED_OUT = constants_1.CONNECTION_CHECKED_OUT;
/**
 * Emitted each time a connection is successfully checked into the connection pool
 * @event
 */ ConnectionPool.CONNECTION_CHECKED_IN = constants_1.CONNECTION_CHECKED_IN; //# sourceMappingURL=connection_pool.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_aws.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoDBAWS = void 0;
const BSON = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const deps_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/deps.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const auth_provider_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [app-route] (ecmascript)");
const aws_temporary_credentials_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/aws_temporary_credentials.js [app-route] (ecmascript)");
const mongo_credentials_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongo_credentials.js [app-route] (ecmascript)");
const providers_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [app-route] (ecmascript)");
const ASCII_N = 110;
const bsonOptions = {
    useBigInt64: false,
    promoteLongs: true,
    promoteValues: true,
    promoteBuffers: false,
    bsonRegExp: false
};
class MongoDBAWS extends auth_provider_1.AuthProvider {
    constructor(credentialProvider){
        super();
        this.credentialProvider = credentialProvider;
        this.credentialFetcher = aws_temporary_credentials_1.AWSTemporaryCredentialProvider.isAWSSDKInstalled ? new aws_temporary_credentials_1.AWSSDKCredentialProvider(credentialProvider) : new aws_temporary_credentials_1.LegacyAWSTemporaryCredentialProvider();
    }
    async auth(authContext) {
        const { connection } = authContext;
        if (!authContext.credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        if ('kModuleError' in deps_1.aws4) {
            throw deps_1.aws4['kModuleError'];
        }
        const { sign } = deps_1.aws4;
        if ((0, utils_1.maxWireVersion)(connection) < 9) {
            throw new error_1.MongoCompatibilityError('MONGODB-AWS authentication requires MongoDB version 4.4 or later');
        }
        if (!authContext.credentials.username) {
            authContext.credentials = await makeTempCredentials(authContext.credentials, this.credentialFetcher);
        }
        const { credentials } = authContext;
        const accessKeyId = credentials.username;
        const secretAccessKey = credentials.password;
        // Allow the user to specify an AWS session token for authentication with temporary credentials.
        const sessionToken = credentials.mechanismProperties.AWS_SESSION_TOKEN;
        // If all three defined, include sessionToken, else include username and pass, else no credentials
        const awsCredentials = accessKeyId && secretAccessKey && sessionToken ? {
            accessKeyId,
            secretAccessKey,
            sessionToken
        } : accessKeyId && secretAccessKey ? {
            accessKeyId,
            secretAccessKey
        } : undefined;
        const db = credentials.source;
        const nonce = await (0, utils_1.randomBytes)(32);
        // All messages between MongoDB clients and servers are sent as BSON objects
        // in the payload field of saslStart and saslContinue.
        const saslStart = {
            saslStart: 1,
            mechanism: 'MONGODB-AWS',
            payload: BSON.serialize({
                r: nonce,
                p: ASCII_N
            }, bsonOptions)
        };
        const saslStartResponse = await connection.command((0, utils_1.ns)(`${db}.$cmd`), saslStart, undefined);
        const serverResponse = BSON.deserialize(saslStartResponse.payload.buffer, bsonOptions);
        const host = serverResponse.h;
        const serverNonce = serverResponse.s.buffer;
        if (serverNonce.length !== 64) {
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError(`Invalid server nonce length ${serverNonce.length}, expected 64`);
        }
        if (!utils_1.ByteUtils.equals(serverNonce.subarray(0, nonce.byteLength), nonce)) {
            // throw because the serverNonce's leading 32 bytes must equal the client nonce's 32 bytes
            // https://github.com/mongodb/specifications/blob/master/source/auth/auth.md#conversation-5
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError('Server nonce does not begin with client nonce');
        }
        if (host.length < 1 || host.length > 255 || host.indexOf('..') !== -1) {
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError(`Server returned an invalid host: "${host}"`);
        }
        const body = 'Action=GetCallerIdentity&Version=2011-06-15';
        const options = sign({
            method: 'POST',
            host,
            region: deriveRegion(serverResponse.h),
            service: 'sts',
            headers: {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Content-Length': body.length,
                'X-MongoDB-Server-Nonce': utils_1.ByteUtils.toBase64(serverNonce),
                'X-MongoDB-GS2-CB-Flag': 'n'
            },
            path: '/',
            body
        }, awsCredentials);
        const payload = {
            a: options.headers.Authorization,
            d: options.headers['X-Amz-Date']
        };
        if (sessionToken) {
            payload.t = sessionToken;
        }
        const saslContinue = {
            saslContinue: 1,
            conversationId: saslStartResponse.conversationId,
            payload: BSON.serialize(payload, bsonOptions)
        };
        await connection.command((0, utils_1.ns)(`${db}.$cmd`), saslContinue, undefined);
    }
}
exports.MongoDBAWS = MongoDBAWS;
async function makeTempCredentials(credentials, awsCredentialFetcher) {
    function makeMongoCredentialsFromAWSTemp(creds) {
        // The AWS session token (creds.Token) may or may not be set.
        if (!creds.AccessKeyId || !creds.SecretAccessKey) {
            throw new error_1.MongoMissingCredentialsError('Could not obtain temporary MONGODB-AWS credentials');
        }
        return new mongo_credentials_1.MongoCredentials({
            username: creds.AccessKeyId,
            password: creds.SecretAccessKey,
            source: credentials.source,
            mechanism: providers_1.AuthMechanism.MONGODB_AWS,
            mechanismProperties: {
                AWS_SESSION_TOKEN: creds.Token
            }
        });
    }
    const temporaryCredentials = await awsCredentialFetcher.getCredentials();
    return makeMongoCredentialsFromAWSTemp(temporaryCredentials);
}
function deriveRegion(host) {
    const parts = host.split('.');
    if (parts.length === 1 || parts[1] === 'amazonaws') {
        return 'us-east-1';
    }
    return parts[1];
} //# sourceMappingURL=mongodb_aws.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/command_builders.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.finishCommandDocument = finishCommandDocument;
exports.startCommandDocument = startCommandDocument;
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const providers_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [app-route] (ecmascript)");
/**
 * Generate the finishing command document for authentication. Will be a
 * saslStart or saslContinue depending on the presence of a conversation id.
 */ function finishCommandDocument(token, conversationId) {
    if (conversationId != null) {
        return {
            saslContinue: 1,
            conversationId: conversationId,
            payload: new bson_1.Binary(bson_1.BSON.serialize({
                jwt: token
            }))
        };
    }
    // saslContinue requires a conversationId in the command to be valid so in this
    // case the server allows "step two" to actually be a saslStart with the token
    // as the jwt since the use of the cached value has no correlating conversating
    // on the particular connection.
    return {
        saslStart: 1,
        mechanism: providers_1.AuthMechanism.MONGODB_OIDC,
        payload: new bson_1.Binary(bson_1.BSON.serialize({
            jwt: token
        }))
    };
}
/**
 * Generate the saslStart command document.
 */ function startCommandDocument(credentials) {
    const payload = {};
    if (credentials.username) {
        payload.n = credentials.username;
    }
    return {
        saslStart: 1,
        autoAuthorize: 1,
        mechanism: providers_1.AuthMechanism.MONGODB_OIDC,
        payload: new bson_1.Binary(bson_1.BSON.serialize(payload))
    };
} //# sourceMappingURL=command_builders.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/callback_workflow.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CallbackWorkflow = exports.AUTOMATED_TIMEOUT_MS = exports.HUMAN_TIMEOUT_MS = void 0;
const promises_1 = require("timers/promises");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const command_builders_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/command_builders.js [app-route] (ecmascript)");
/** 5 minutes in milliseconds */ exports.HUMAN_TIMEOUT_MS = 300000;
/** 1 minute in milliseconds */ exports.AUTOMATED_TIMEOUT_MS = 60000;
/** Properties allowed on results of callbacks. */ const RESULT_PROPERTIES = [
    'accessToken',
    'expiresInSeconds',
    'refreshToken'
];
/** Error message when the callback result is invalid. */ const CALLBACK_RESULT_ERROR = 'User provided OIDC callbacks must return a valid object with an accessToken.';
/** The time to throttle callback calls. */ const THROTTLE_MS = 100;
/**
 * OIDC implementation of a callback based workflow.
 * @internal
 */ class CallbackWorkflow {
    /**
     * Instantiate the callback workflow.
     */ constructor(cache, callback){
        this.cache = cache;
        this.callback = this.withLock(callback);
        this.lastExecutionTime = Date.now() - THROTTLE_MS;
    }
    /**
     * Get the document to add for speculative authentication. This also needs
     * to add a db field from the credentials source.
     */ async speculativeAuth(connection, credentials) {
        // Check if the Client Cache has an access token.
        // If it does, cache the access token in the Connection Cache and send a JwtStepRequest
        // with the cached access token in the speculative authentication SASL payload.
        if (this.cache.hasAccessToken) {
            const accessToken = this.cache.getAccessToken();
            connection.accessToken = accessToken;
            const document = (0, command_builders_1.finishCommandDocument)(accessToken);
            document.db = credentials.source;
            return {
                speculativeAuthenticate: document
            };
        }
        return {};
    }
    /**
     * Reauthenticate the callback workflow. For this we invalidated the access token
     * in the cache and run the authentication steps again. No initial handshake needs
     * to be sent.
     */ async reauthenticate(connection, credentials) {
        if (this.cache.hasAccessToken) {
            // Reauthentication implies the token has expired.
            if (connection.accessToken === this.cache.getAccessToken()) {
                // If connection's access token is the same as the cache's, remove
                // the token from the cache and connection.
                this.cache.removeAccessToken();
                delete connection.accessToken;
            } else {
                // If the connection's access token is different from the cache's, set
                // the cache's token on the connection and do not remove from the
                // cache.
                connection.accessToken = this.cache.getAccessToken();
            }
        }
        await this.execute(connection, credentials);
    }
    /**
     * Starts the callback authentication process. If there is a speculative
     * authentication document from the initial handshake, then we will use that
     * value to get the issuer, otherwise we will send the saslStart command.
     */ async startAuthentication(connection, credentials, response) {
        let result;
        if (response?.speculativeAuthenticate) {
            result = response.speculativeAuthenticate;
        } else {
            result = await connection.command((0, utils_1.ns)(credentials.source), (0, command_builders_1.startCommandDocument)(credentials), undefined);
        }
        return result;
    }
    /**
     * Finishes the callback authentication process.
     */ async finishAuthentication(connection, credentials, token, conversationId) {
        await connection.command((0, utils_1.ns)(credentials.source), (0, command_builders_1.finishCommandDocument)(token, conversationId), undefined);
    }
    /**
     * Executes the callback and validates the output.
     */ async executeAndValidateCallback(params) {
        const result = await this.callback(params);
        // Validate that the result returned by the callback is acceptable. If it is not
        // we must clear the token result from the cache.
        if (isCallbackResultInvalid(result)) {
            throw new error_1.MongoMissingCredentialsError(CALLBACK_RESULT_ERROR);
        }
        return result;
    }
    /**
     * Ensure the callback is only executed one at a time and throttles the calls
     * to every 100ms.
     */ withLock(callback) {
        let lock = Promise.resolve();
        return async (params)=>{
            // We do this to ensure that we would never return the result of the
            // previous lock, only the current callback's value would get returned.
            await lock;
            lock = lock.catch(()=>null).then(async ()=>{
                const difference = Date.now() - this.lastExecutionTime;
                if (difference <= THROTTLE_MS) {
                    await (0, promises_1.setTimeout)(THROTTLE_MS - difference, {
                        signal: params.timeoutContext
                    });
                }
                this.lastExecutionTime = Date.now();
                return await callback(params);
            });
            return await lock;
        };
    }
}
exports.CallbackWorkflow = CallbackWorkflow;
/**
 * Determines if a result returned from a request or refresh callback
 * function is invalid. This means the result is nullish, doesn't contain
 * the accessToken required field, and does not contain extra fields.
 */ function isCallbackResultInvalid(tokenResult) {
    if (tokenResult == null || typeof tokenResult !== 'object') return true;
    if (!('accessToken' in tokenResult)) return true;
    return !Object.getOwnPropertyNames(tokenResult).every((prop)=>RESULT_PROPERTIES.includes(prop));
} //# sourceMappingURL=callback_workflow.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/automated_callback_workflow.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AutomatedCallbackWorkflow = void 0;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const timeout_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/timeout.js [app-route] (ecmascript)");
const mongodb_oidc_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc.js [app-route] (ecmascript)");
const callback_workflow_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/callback_workflow.js [app-route] (ecmascript)");
/**
 * Class implementing behaviour for the non human callback workflow.
 * @internal
 */ class AutomatedCallbackWorkflow extends callback_workflow_1.CallbackWorkflow {
    /**
     * Instantiate the human callback workflow.
     */ constructor(cache, callback){
        super(cache, callback);
    }
    /**
     * Execute the OIDC callback workflow.
     */ async execute(connection, credentials) {
        // If there is a cached access token, try to authenticate with it. If
        // authentication fails with an Authentication error (18),
        // invalidate the access token, fetch a new access token, and try
        // to authenticate again.
        // If the server fails for any other reason, do not clear the cache.
        if (this.cache.hasAccessToken) {
            const token = this.cache.getAccessToken();
            if (!connection.accessToken) {
                connection.accessToken = token;
            }
            try {
                return await this.finishAuthentication(connection, credentials, token);
            } catch (error) {
                if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.AuthenticationFailed) {
                    this.cache.removeAccessToken();
                    return await this.execute(connection, credentials);
                } else {
                    throw error;
                }
            }
        }
        const response = await this.fetchAccessToken(credentials);
        this.cache.put(response);
        connection.accessToken = response.accessToken;
        await this.finishAuthentication(connection, credentials, response.accessToken);
    }
    /**
     * Fetches the access token using the callback.
     */ async fetchAccessToken(credentials) {
        const controller = new AbortController();
        const params = {
            timeoutContext: controller.signal,
            version: mongodb_oidc_1.OIDC_VERSION
        };
        if (credentials.username) {
            params.username = credentials.username;
        }
        if (credentials.mechanismProperties.TOKEN_RESOURCE) {
            params.tokenAudience = credentials.mechanismProperties.TOKEN_RESOURCE;
        }
        const timeout = timeout_1.Timeout.expires(callback_workflow_1.AUTOMATED_TIMEOUT_MS);
        try {
            return await Promise.race([
                this.executeAndValidateCallback(params),
                timeout
            ]);
        } catch (error) {
            if (timeout_1.TimeoutError.is(error)) {
                controller.abort();
                throw new error_1.MongoOIDCError(`OIDC callback timed out after ${callback_workflow_1.AUTOMATED_TIMEOUT_MS}ms.`);
            }
            throw error;
        } finally{
            timeout.clear();
        }
    }
}
exports.AutomatedCallbackWorkflow = AutomatedCallbackWorkflow; //# sourceMappingURL=automated_callback_workflow.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/azure_machine_workflow.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.callback = void 0;
const azure_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/client-side-encryption/providers/azure.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
/** Azure request headers. */ const AZURE_HEADERS = Object.freeze({
    Metadata: 'true',
    Accept: 'application/json'
});
/** Invalid endpoint result error. */ const ENDPOINT_RESULT_ERROR = 'Azure endpoint did not return a value with only access_token and expires_in properties';
/** Error for when the token audience is missing in the environment. */ const TOKEN_RESOURCE_MISSING_ERROR = 'TOKEN_RESOURCE must be set in the auth mechanism properties when ENVIRONMENT is azure.';
/**
 * The callback function to be used in the automated callback workflow.
 * @param params - The OIDC callback parameters.
 * @returns The OIDC response.
 */ const callback = async (params)=>{
    const tokenAudience = params.tokenAudience;
    const username = params.username;
    if (!tokenAudience) {
        throw new error_1.MongoAzureError(TOKEN_RESOURCE_MISSING_ERROR);
    }
    const response = await getAzureTokenData(tokenAudience, username);
    if (!isEndpointResultValid(response)) {
        throw new error_1.MongoAzureError(ENDPOINT_RESULT_ERROR);
    }
    return response;
};
exports.callback = callback;
/**
 * Hit the Azure endpoint to get the token data.
 */ async function getAzureTokenData(tokenAudience, username) {
    const url = new URL(azure_1.AZURE_BASE_URL);
    (0, azure_1.addAzureParams)(url, tokenAudience, username);
    const response = await (0, utils_1.get)(url, {
        headers: AZURE_HEADERS
    });
    if (response.status !== 200) {
        throw new error_1.MongoAzureError(`Status code ${response.status} returned from the Azure endpoint. Response body: ${response.body}`);
    }
    const result = JSON.parse(response.body);
    return {
        accessToken: result.access_token,
        expiresInSeconds: Number(result.expires_in)
    };
}
/**
 * Determines if a result returned from the endpoint is valid.
 * This means the result is not nullish, contains the access_token required field
 * and the expires_in required field.
 */ function isEndpointResultValid(token) {
    if (token == null || typeof token !== 'object') return false;
    return 'accessToken' in token && typeof token.accessToken === 'string' && 'expiresInSeconds' in token && typeof token.expiresInSeconds === 'number';
} //# sourceMappingURL=azure_machine_workflow.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/gcp_machine_workflow.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.callback = void 0;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
/** GCP base URL. */ const GCP_BASE_URL = 'http://metadata/computeMetadata/v1/instance/service-accounts/default/identity';
/** GCP request headers. */ const GCP_HEADERS = Object.freeze({
    'Metadata-Flavor': 'Google'
});
/** Error for when the token audience is missing in the environment. */ const TOKEN_RESOURCE_MISSING_ERROR = 'TOKEN_RESOURCE must be set in the auth mechanism properties when ENVIRONMENT is gcp.';
/**
 * The callback function to be used in the automated callback workflow.
 * @param params - The OIDC callback parameters.
 * @returns The OIDC response.
 */ const callback = async (params)=>{
    const tokenAudience = params.tokenAudience;
    if (!tokenAudience) {
        throw new error_1.MongoGCPError(TOKEN_RESOURCE_MISSING_ERROR);
    }
    return await getGcpTokenData(tokenAudience);
};
exports.callback = callback;
/**
 * Hit the GCP endpoint to get the token data.
 */ async function getGcpTokenData(tokenAudience) {
    const url = new URL(GCP_BASE_URL);
    url.searchParams.append('audience', tokenAudience);
    const response = await (0, utils_1.get)(url, {
        headers: GCP_HEADERS
    });
    if (response.status !== 200) {
        throw new error_1.MongoGCPError(`Status code ${response.status} returned from the GCP endpoint. Response body: ${response.body}`);
    }
    return {
        accessToken: response.body
    };
} //# sourceMappingURL=gcp_machine_workflow.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/k8s_machine_workflow.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.callback = void 0;
const promises_1 = require("fs/promises");
/** The fallback file name */ const FALLBACK_FILENAME = '/var/run/secrets/kubernetes.io/serviceaccount/token';
/** The azure environment variable for the file name. */ const AZURE_FILENAME = 'AZURE_FEDERATED_TOKEN_FILE';
/** The AWS environment variable for the file name. */ const AWS_FILENAME = 'AWS_WEB_IDENTITY_TOKEN_FILE';
/**
 * The callback function to be used in the automated callback workflow.
 * @param params - The OIDC callback parameters.
 * @returns The OIDC response.
 */ const callback = async ()=>{
    let filename;
    if (process.env[AZURE_FILENAME]) {
        filename = process.env[AZURE_FILENAME];
    } else if (process.env[AWS_FILENAME]) {
        filename = process.env[AWS_FILENAME];
    } else {
        filename = FALLBACK_FILENAME;
    }
    const token = await (0, promises_1.readFile)(filename, 'utf8');
    return {
        accessToken: token
    };
};
exports.callback = callback; //# sourceMappingURL=k8s_machine_workflow.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_cache.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.TokenCache = void 0;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
class MongoOIDCError extends error_1.MongoDriverError {
}
/** @internal */ class TokenCache {
    get hasAccessToken() {
        return !!this.accessToken;
    }
    get hasRefreshToken() {
        return !!this.refreshToken;
    }
    get hasIdpInfo() {
        return !!this.idpInfo;
    }
    getAccessToken() {
        if (!this.accessToken) {
            throw new MongoOIDCError('Attempted to get an access token when none exists.');
        }
        return this.accessToken;
    }
    getRefreshToken() {
        if (!this.refreshToken) {
            throw new MongoOIDCError('Attempted to get a refresh token when none exists.');
        }
        return this.refreshToken;
    }
    getIdpInfo() {
        if (!this.idpInfo) {
            throw new MongoOIDCError('Attempted to get IDP information when none exists.');
        }
        return this.idpInfo;
    }
    put(response, idpInfo) {
        this.accessToken = response.accessToken;
        this.refreshToken = response.refreshToken;
        this.expiresInSeconds = response.expiresInSeconds;
        if (idpInfo) {
            this.idpInfo = idpInfo;
        }
    }
    removeAccessToken() {
        this.accessToken = undefined;
    }
    removeRefreshToken() {
        this.refreshToken = undefined;
    }
}
exports.TokenCache = TokenCache; //# sourceMappingURL=token_cache.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_machine_workflow.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.callback = void 0;
const fs = require("fs");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
/** Error for when the token is missing in the environment. */ const TOKEN_MISSING_ERROR = 'OIDC_TOKEN_FILE must be set in the environment.';
/**
 * The callback function to be used in the automated callback workflow.
 * @param params - The OIDC callback parameters.
 * @returns The OIDC response.
 */ const callback = async ()=>{
    const tokenFile = process.env.OIDC_TOKEN_FILE;
    if (!tokenFile) {
        throw new error_1.MongoAWSError(TOKEN_MISSING_ERROR);
    }
    const token = await fs.promises.readFile(tokenFile, 'utf8');
    return {
        accessToken: token
    };
};
exports.callback = callback; //# sourceMappingURL=token_machine_workflow.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoDBOIDC = exports.OIDC_WORKFLOWS = exports.OIDC_VERSION = void 0;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const auth_provider_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [app-route] (ecmascript)");
const automated_callback_workflow_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/automated_callback_workflow.js [app-route] (ecmascript)");
const azure_machine_workflow_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/azure_machine_workflow.js [app-route] (ecmascript)");
const gcp_machine_workflow_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/gcp_machine_workflow.js [app-route] (ecmascript)");
const k8s_machine_workflow_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/k8s_machine_workflow.js [app-route] (ecmascript)");
const token_cache_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_cache.js [app-route] (ecmascript)");
const token_machine_workflow_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_machine_workflow.js [app-route] (ecmascript)");
/** Error when credentials are missing. */ const MISSING_CREDENTIALS_ERROR = 'AuthContext must provide credentials.';
/** The current version of OIDC implementation. */ exports.OIDC_VERSION = 1;
/** @internal */ exports.OIDC_WORKFLOWS = new Map();
exports.OIDC_WORKFLOWS.set('test', ()=>new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), token_machine_workflow_1.callback));
exports.OIDC_WORKFLOWS.set('azure', ()=>new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), azure_machine_workflow_1.callback));
exports.OIDC_WORKFLOWS.set('gcp', ()=>new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), gcp_machine_workflow_1.callback));
exports.OIDC_WORKFLOWS.set('k8s', ()=>new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), k8s_machine_workflow_1.callback));
/**
 * OIDC auth provider.
 */ class MongoDBOIDC extends auth_provider_1.AuthProvider {
    /**
     * Instantiate the auth provider.
     */ constructor(workflow){
        super();
        if (!workflow) {
            throw new error_1.MongoInvalidArgumentError('No workflow provided to the OIDC auth provider.');
        }
        this.workflow = workflow;
    }
    /**
     * Authenticate using OIDC
     */ async auth(authContext) {
        const { connection, reauthenticating, response } = authContext;
        if (response?.speculativeAuthenticate?.done && !reauthenticating) {
            return;
        }
        const credentials = getCredentials(authContext);
        if (reauthenticating) {
            await this.workflow.reauthenticate(connection, credentials);
        } else {
            await this.workflow.execute(connection, credentials, response);
        }
    }
    /**
     * Add the speculative auth for the initial handshake.
     */ async prepare(handshakeDoc, authContext) {
        const { connection } = authContext;
        const credentials = getCredentials(authContext);
        const result = await this.workflow.speculativeAuth(connection, credentials);
        return {
            ...handshakeDoc,
            ...result
        };
    }
}
exports.MongoDBOIDC = MongoDBOIDC;
/**
 * Get credentials from the auth context, throwing if they do not exist.
 */ function getCredentials(authContext) {
    const { credentials } = authContext;
    if (!credentials) {
        throw new error_1.MongoMissingCredentialsError(MISSING_CREDENTIALS_ERROR);
    }
    return credentials;
} //# sourceMappingURL=mongodb_oidc.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/human_callback_workflow.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.HumanCallbackWorkflow = void 0;
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const timeout_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/timeout.js [app-route] (ecmascript)");
const mongodb_oidc_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc.js [app-route] (ecmascript)");
const callback_workflow_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/callback_workflow.js [app-route] (ecmascript)");
/**
 * Class implementing behaviour for the non human callback workflow.
 * @internal
 */ class HumanCallbackWorkflow extends callback_workflow_1.CallbackWorkflow {
    /**
     * Instantiate the human callback workflow.
     */ constructor(cache, callback){
        super(cache, callback);
    }
    /**
     * Execute the OIDC human callback workflow.
     */ async execute(connection, credentials) {
        // Check if the Client Cache has an access token.
        // If it does, cache the access token in the Connection Cache and perform a One-Step SASL conversation
        // using the access token. If the server returns an Authentication error (18),
        // invalidate the access token token from the Client Cache, clear the Connection Cache,
        // and restart the authentication flow. Raise any other errors to the user. On success, exit the algorithm.
        if (this.cache.hasAccessToken) {
            const token = this.cache.getAccessToken();
            connection.accessToken = token;
            try {
                return await this.finishAuthentication(connection, credentials, token);
            } catch (error) {
                if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.AuthenticationFailed) {
                    this.cache.removeAccessToken();
                    delete connection.accessToken;
                    return await this.execute(connection, credentials);
                } else {
                    throw error;
                }
            }
        }
        // Check if the Client Cache has a refresh token.
        // If it does, call the OIDC Human Callback with the cached refresh token and IdpInfo to get a
        // new access token. Cache the new access token in the Client Cache and Connection Cache.
        // Perform a One-Step SASL conversation using the new access token. If the the server returns
        // an Authentication error (18), clear the refresh token, invalidate the access token from the
        // Client Cache, clear the Connection Cache, and restart the authentication flow. Raise any other
        // errors to the user. On success, exit the algorithm.
        if (this.cache.hasRefreshToken) {
            const refreshToken = this.cache.getRefreshToken();
            const result = await this.fetchAccessToken(this.cache.getIdpInfo(), credentials, refreshToken);
            this.cache.put(result);
            connection.accessToken = result.accessToken;
            try {
                return await this.finishAuthentication(connection, credentials, result.accessToken);
            } catch (error) {
                if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.AuthenticationFailed) {
                    this.cache.removeRefreshToken();
                    delete connection.accessToken;
                    return await this.execute(connection, credentials);
                } else {
                    throw error;
                }
            }
        }
        // Start a new Two-Step SASL conversation.
        // Run a PrincipalStepRequest to get the IdpInfo.
        // Call the OIDC Human Callback with the new IdpInfo to get a new access token and optional refresh
        // token. Drivers MUST NOT pass a cached refresh token to the callback when performing
        // a new Two-Step conversation. Cache the new IdpInfo and refresh token in the Client Cache and the
        // new access token in the Client Cache and Connection Cache.
        // Attempt to authenticate using a JwtStepRequest with the new access token. Raise any errors to the user.
        const startResponse = await this.startAuthentication(connection, credentials);
        const conversationId = startResponse.conversationId;
        const idpInfo = bson_1.BSON.deserialize(startResponse.payload.buffer);
        const callbackResponse = await this.fetchAccessToken(idpInfo, credentials);
        this.cache.put(callbackResponse, idpInfo);
        connection.accessToken = callbackResponse.accessToken;
        return await this.finishAuthentication(connection, credentials, callbackResponse.accessToken, conversationId);
    }
    /**
     * Fetches an access token using the callback.
     */ async fetchAccessToken(idpInfo, credentials, refreshToken) {
        const controller = new AbortController();
        const params = {
            timeoutContext: controller.signal,
            version: mongodb_oidc_1.OIDC_VERSION,
            idpInfo: idpInfo
        };
        if (credentials.username) {
            params.username = credentials.username;
        }
        if (refreshToken) {
            params.refreshToken = refreshToken;
        }
        const timeout = timeout_1.Timeout.expires(callback_workflow_1.HUMAN_TIMEOUT_MS);
        try {
            return await Promise.race([
                this.executeAndValidateCallback(params),
                timeout
            ]);
        } catch (error) {
            if (timeout_1.TimeoutError.is(error)) {
                controller.abort();
                throw new error_1.MongoOIDCError(`OIDC callback timed out after ${callback_workflow_1.HUMAN_TIMEOUT_MS}ms.`);
            }
            throw error;
        } finally{
            timeout.clear();
        }
    }
}
exports.HumanCallbackWorkflow = HumanCallbackWorkflow; //# sourceMappingURL=human_callback_workflow.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/plain.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Plain = void 0;
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const auth_provider_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [app-route] (ecmascript)");
class Plain extends auth_provider_1.AuthProvider {
    async auth(authContext) {
        const { connection, credentials } = authContext;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        const { username, password } = credentials;
        const payload = new bson_1.Binary(Buffer.from(`\x00${username}\x00${password}`));
        const command = {
            saslStart: 1,
            mechanism: 'PLAIN',
            payload: payload,
            autoAuthorize: 1
        };
        await connection.command((0, utils_1.ns)('$external.$cmd'), command, undefined);
    }
}
exports.Plain = Plain; //# sourceMappingURL=plain.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/scram.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ScramSHA256 = exports.ScramSHA1 = void 0;
const saslprep_1 = __turbopack_require__("[project]/node_modules/@mongodb-js/saslprep/dist/node.js [app-route] (ecmascript)");
const crypto = require("crypto");
const bson_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/bson.js [app-route] (ecmascript)");
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const auth_provider_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [app-route] (ecmascript)");
const providers_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [app-route] (ecmascript)");
class ScramSHA extends auth_provider_1.AuthProvider {
    constructor(cryptoMethod){
        super();
        this.cryptoMethod = cryptoMethod || 'sha1';
    }
    async prepare(handshakeDoc, authContext) {
        const cryptoMethod = this.cryptoMethod;
        const credentials = authContext.credentials;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        const nonce = await (0, utils_1.randomBytes)(24);
        // store the nonce for later use
        authContext.nonce = nonce;
        const request = {
            ...handshakeDoc,
            speculativeAuthenticate: {
                ...makeFirstMessage(cryptoMethod, credentials, nonce),
                db: credentials.source
            }
        };
        return request;
    }
    async auth(authContext) {
        const { reauthenticating, response } = authContext;
        if (response?.speculativeAuthenticate && !reauthenticating) {
            return await continueScramConversation(this.cryptoMethod, response.speculativeAuthenticate, authContext);
        }
        return await executeScram(this.cryptoMethod, authContext);
    }
}
function cleanUsername(username) {
    return username.replace('=', '=3D').replace(',', '=2C');
}
function clientFirstMessageBare(username, nonce) {
    // NOTE: This is done b/c Javascript uses UTF-16, but the server is hashing in UTF-8.
    // Since the username is not sasl-prep-d, we need to do this here.
    return Buffer.concat([
        Buffer.from('n=', 'utf8'),
        Buffer.from(username, 'utf8'),
        Buffer.from(',r=', 'utf8'),
        Buffer.from(nonce.toString('base64'), 'utf8')
    ]);
}
function makeFirstMessage(cryptoMethod, credentials, nonce) {
    const username = cleanUsername(credentials.username);
    const mechanism = cryptoMethod === 'sha1' ? providers_1.AuthMechanism.MONGODB_SCRAM_SHA1 : providers_1.AuthMechanism.MONGODB_SCRAM_SHA256;
    // NOTE: This is done b/c Javascript uses UTF-16, but the server is hashing in UTF-8.
    // Since the username is not sasl-prep-d, we need to do this here.
    return {
        saslStart: 1,
        mechanism,
        payload: new bson_1.Binary(Buffer.concat([
            Buffer.from('n,,', 'utf8'),
            clientFirstMessageBare(username, nonce)
        ])),
        autoAuthorize: 1,
        options: {
            skipEmptyExchange: true
        }
    };
}
async function executeScram(cryptoMethod, authContext) {
    const { connection, credentials } = authContext;
    if (!credentials) {
        throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
    }
    if (!authContext.nonce) {
        throw new error_1.MongoInvalidArgumentError('AuthContext must contain a valid nonce property');
    }
    const nonce = authContext.nonce;
    const db = credentials.source;
    const saslStartCmd = makeFirstMessage(cryptoMethod, credentials, nonce);
    const response = await connection.command((0, utils_1.ns)(`${db}.$cmd`), saslStartCmd, undefined);
    await continueScramConversation(cryptoMethod, response, authContext);
}
async function continueScramConversation(cryptoMethod, response, authContext) {
    const connection = authContext.connection;
    const credentials = authContext.credentials;
    if (!credentials) {
        throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
    }
    if (!authContext.nonce) {
        throw new error_1.MongoInvalidArgumentError('Unable to continue SCRAM without valid nonce');
    }
    const nonce = authContext.nonce;
    const db = credentials.source;
    const username = cleanUsername(credentials.username);
    const password = credentials.password;
    const processedPassword = cryptoMethod === 'sha256' ? (0, saslprep_1.saslprep)(password) : passwordDigest(username, password);
    const payload = Buffer.isBuffer(response.payload) ? new bson_1.Binary(response.payload) : response.payload;
    const dict = parsePayload(payload);
    const iterations = parseInt(dict.i, 10);
    if (iterations && iterations < 4096) {
        // TODO(NODE-3483)
        throw new error_1.MongoRuntimeError(`Server returned an invalid iteration count ${iterations}`);
    }
    const salt = dict.s;
    const rnonce = dict.r;
    if (rnonce.startsWith('nonce')) {
        // TODO(NODE-3483)
        throw new error_1.MongoRuntimeError(`Server returned an invalid nonce: ${rnonce}`);
    }
    // Set up start of proof
    const withoutProof = `c=biws,r=${rnonce}`;
    const saltedPassword = HI(processedPassword, Buffer.from(salt, 'base64'), iterations, cryptoMethod);
    const clientKey = HMAC(cryptoMethod, saltedPassword, 'Client Key');
    const serverKey = HMAC(cryptoMethod, saltedPassword, 'Server Key');
    const storedKey = H(cryptoMethod, clientKey);
    const authMessage = [
        clientFirstMessageBare(username, nonce),
        payload.toString('utf8'),
        withoutProof
    ].join(',');
    const clientSignature = HMAC(cryptoMethod, storedKey, authMessage);
    const clientProof = `p=${xor(clientKey, clientSignature)}`;
    const clientFinal = [
        withoutProof,
        clientProof
    ].join(',');
    const serverSignature = HMAC(cryptoMethod, serverKey, authMessage);
    const saslContinueCmd = {
        saslContinue: 1,
        conversationId: response.conversationId,
        payload: new bson_1.Binary(Buffer.from(clientFinal))
    };
    const r = await connection.command((0, utils_1.ns)(`${db}.$cmd`), saslContinueCmd, undefined);
    const parsedResponse = parsePayload(r.payload);
    if (!compareDigest(Buffer.from(parsedResponse.v, 'base64'), serverSignature)) {
        throw new error_1.MongoRuntimeError('Server returned an invalid signature');
    }
    if (r.done !== false) {
        // If the server sends r.done === true we can save one RTT
        return;
    }
    const retrySaslContinueCmd = {
        saslContinue: 1,
        conversationId: r.conversationId,
        payload: Buffer.alloc(0)
    };
    await connection.command((0, utils_1.ns)(`${db}.$cmd`), retrySaslContinueCmd, undefined);
}
function parsePayload(payload) {
    const payloadStr = payload.toString('utf8');
    const dict = {};
    const parts = payloadStr.split(',');
    for(let i = 0; i < parts.length; i++){
        const valueParts = (parts[i].match(/^([^=]*)=(.*)$/) ?? []).slice(1);
        dict[valueParts[0]] = valueParts[1];
    }
    return dict;
}
function passwordDigest(username, password) {
    if (typeof username !== 'string') {
        throw new error_1.MongoInvalidArgumentError('Username must be a string');
    }
    if (typeof password !== 'string') {
        throw new error_1.MongoInvalidArgumentError('Password must be a string');
    }
    if (password.length === 0) {
        throw new error_1.MongoInvalidArgumentError('Password cannot be empty');
    }
    let md5;
    try {
        md5 = crypto.createHash('md5');
    } catch (err) {
        if (crypto.getFips()) {
            // This error is (slightly) more helpful than what comes from OpenSSL directly, e.g.
            // 'Error: error:060800C8:digital envelope routines:EVP_DigestInit_ex:disabled for FIPS'
            throw new Error('Auth mechanism SCRAM-SHA-1 is not supported in FIPS mode');
        }
        throw err;
    }
    md5.update(`${username}:mongo:${password}`, 'utf8');
    return md5.digest('hex');
}
// XOR two buffers
function xor(a, b) {
    if (!Buffer.isBuffer(a)) {
        a = Buffer.from(a);
    }
    if (!Buffer.isBuffer(b)) {
        b = Buffer.from(b);
    }
    const length = Math.max(a.length, b.length);
    const res = [];
    for(let i = 0; i < length; i += 1){
        res.push(a[i] ^ b[i]);
    }
    return Buffer.from(res).toString('base64');
}
function H(method, text) {
    return crypto.createHash(method).update(text).digest();
}
function HMAC(method, key, text) {
    return crypto.createHmac(method, key).update(text).digest();
}
let _hiCache = {};
let _hiCacheCount = 0;
function _hiCachePurge() {
    _hiCache = {};
    _hiCacheCount = 0;
}
const hiLengthMap = {
    sha256: 32,
    sha1: 20
};
function HI(data, salt, iterations, cryptoMethod) {
    // omit the work if already generated
    const key = [
        data,
        salt.toString('base64'),
        iterations
    ].join('_');
    if (_hiCache[key] != null) {
        return _hiCache[key];
    }
    // generate the salt
    const saltedData = crypto.pbkdf2Sync(data, salt, iterations, hiLengthMap[cryptoMethod], cryptoMethod);
    // cache a copy to speed up the next lookup, but prevent unbounded cache growth
    if (_hiCacheCount >= 200) {
        _hiCachePurge();
    }
    _hiCache[key] = saltedData;
    _hiCacheCount += 1;
    return saltedData;
}
function compareDigest(lhs, rhs) {
    if (lhs.length !== rhs.length) {
        return false;
    }
    if (typeof crypto.timingSafeEqual === 'function') {
        return crypto.timingSafeEqual(lhs, rhs);
    }
    let result = 0;
    for(let i = 0; i < lhs.length; i++){
        result |= lhs[i] ^ rhs[i];
    }
    return result === 0;
}
class ScramSHA1 extends ScramSHA {
    constructor(){
        super('sha1');
    }
}
exports.ScramSHA1 = ScramSHA1;
class ScramSHA256 extends ScramSHA {
    constructor(){
        super('sha256');
    }
}
exports.ScramSHA256 = ScramSHA256; //# sourceMappingURL=scram.js.map

}.call(this) }),
"[project]/node_modules/mongodb/lib/cmap/auth/x509.js [app-route] (ecmascript)": (function({ r: __turbopack_require__, f: __turbopack_module_context__, i: __turbopack_import__, s: __turbopack_esm__, v: __turbopack_export_value__, n: __turbopack_export_namespace__, c: __turbopack_cache__, M: __turbopack_modules__, l: __turbopack_load__, j: __turbopack_dynamic__, P: __turbopack_resolve_absolute_path__, U: __turbopack_relative_url__, R: __turbopack_resolve_module_id_path__, g: global, __dirname, x: __turbopack_external_require__, y: __turbopack_external_import__, m: module, e: exports, t: require }) { !function() {

"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.X509 = void 0;
const error_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/error.js [app-route] (ecmascript)");
const utils_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/utils.js [app-route] (ecmascript)");
const auth_provider_1 = __turbopack_require__("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [app-route] (ecmascript)");
class X509 extends auth_provider_1.AuthProvider {
    async prepare(handshakeDoc, authContext) {
        const { credentials } = authContext;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        return {
            ...handshakeDoc,
            speculativeAuthenticate: x509AuthenticateCommand(credentials)
        };
    }
    async auth(authContext) {
        const connection = authContext.connection;
        const credentials = authContext.credentials;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        const response = authContext.response;
        if (response?.speculativeAuthenticate) {
            return;
        }
        await connection.command((0, utils_1.ns)('$external.$cmd'), x509AuthenticateCommand(credentials), undefined);
    }
}
exports.X509 = X509;
function x509AuthenticateCommand(credentials) {
    const command = {
        authenticate: 1,
        mechanism: 'MONGODB-X509'
    };
    if (credentials.username) {
        command.user = credentials.username;
    }
    return command;
} //# sourceMappingURL=x509.js.map

}.call(this) }),

};

//# sourceMappingURL=node_modules_mongodb_lib_cmap_62db2f._.js.map